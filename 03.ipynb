{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "all_data=data/SICK/SICK_all.txt\n",
      "char_embedding_dim=50\n",
      "cnt=10\n",
      "data_path=data\n",
      "min_word_freq=2\n",
      "model_name=03\n",
      "result_path=result/\n",
      "save_path=model/\n",
      "test_data=data/SICK/SICK_test_annotated.txt\n",
      "train_data=data/SICK/SICK_new_train.txt\n",
      "use_fp64=False\n",
      "validation_data=data/SICK/SICK_new_trial.txt\n",
      "word2vec_path=embeddings/GoogleNews-vectors-negative300.txt\n",
      "word_embedding_dim=300\n",
      "2556 54\n",
      "Load word2vec_norm file embeddings/GoogleNews-vectors-negative300.txt\n",
      "2556 300\n",
      "vocab_size=2556\n",
      "preloaded vectors\n",
      "dataset read\n",
      "32\n",
      "16\n",
      "pad_to_max_len_sentence_and_word_len\n",
      "build_model\n",
      "training..\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import collections\n",
    "import re\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "#comment: for server only\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "\n",
    "# Model Hyperparameters\n",
    "flags=tf.flags\n",
    "\n",
    "flags.DEFINE_string('word2vec_path','embeddings/GoogleNews-vectors-negative300.txt','Word2vec file with pre-trained embeddings')\n",
    "flags.DEFINE_string('data_path','data','data set path')\n",
    "flags.DEFINE_string('min_word_freq','2','Minimum word frequency')\n",
    "flags.DEFINE_string('save_path','model/','STS model output directory')\n",
    "flags.DEFINE_string('result_path','result/','data set path')\n",
    "flags.DEFINE_string('test_data',\"data/SICK/SICK_test_annotated.txt\",'Test data')\n",
    "flags.DEFINE_string('validation_data',\"data/SICK/SICK_new_trial.txt\",'Validation data')\n",
    "flags.DEFINE_string('train_data',\"data/SICK/SICK_new_train.txt\",'Train data')\n",
    "flags.DEFINE_string('all_data',\"data/SICK/SICK_all.txt\",'Train data')\n",
    "flags.DEFINE_string('cnt',\"10\",'Number of samples to show')\n",
    "flags.DEFINE_string('model_name','03','Filename of the model file')\n",
    "flags.DEFINE_integer('word_embedding_dim',300,'Dimensionality of word embedding')\n",
    "flags.DEFINE_integer('char_embedding_dim',50,'Dimensionality of char embedding')\n",
    "flags.DEFINE_bool('use_fp64',False,'Train using 64-bit floats instead of 32bit floats')\n",
    "\n",
    "\n",
    "\n",
    "FLAGS=flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print('Parameters:')\n",
    "for attr,value in sorted(FLAGS.__flags.items()):\n",
    "    print('{}={}'.format(attr,value))\n",
    "    \n",
    "    \n",
    "\n",
    "# model parameters\n",
    "class Config(object):\n",
    "    init_scale=0.2\n",
    "    learning_rate=.01\n",
    "    max_grad_norm=1.\n",
    "    keep_prob=0.5\n",
    "    lr_decay=0.98\n",
    "    batch_size=30\n",
    "    lr_max_epoch=8##this is for learning rate epoch\n",
    "    train_max_epoch=100\n",
    "    num_layer=1\n",
    "    num_units=50\n",
    "    patience=5\n",
    "    \n",
    "config=Config()\n",
    "config_gpu = tf.ConfigProto()\n",
    "config_gpu.gpu_options.allow_growth = True\n",
    "    \n",
    "def data_type():\n",
    "    return tf.float64 if FLAGS.use_fp64 else tf.float32\n",
    "\n",
    "\n",
    "\n",
    "class Input(object):\n",
    "    def __init__(self,sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores):\n",
    "        self.sentences_A=sentences_A\n",
    "        self.sentencesA_length=sentencesA_length\n",
    "        self.sentences_B=sentences_B\n",
    "        self.sentencesB_length=sentencesB_length\n",
    "        self.relatedness_scores=relatedness_scores\n",
    "    \n",
    "    def sentences_A(self):\n",
    "        return self.sentences_A\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesA_length\n",
    "    \n",
    "    def sentences_B(self):\n",
    "        return self.sentences_B\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesB_length\n",
    "    \n",
    "    def relatedness_scores(self):\n",
    "        return self.relatedness_scores\n",
    "\n",
    "\n",
    "'''\n",
    "Reads SICK file. Take note of the header line.\n",
    "'''\n",
    "def read_input_file(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        f.readline() # removes header line/column labels\n",
    "        sentences_A = []\n",
    "        sentencesA_length = []\n",
    "        sentences_B = []\n",
    "        sentencesB_length = []\n",
    "        relatedness_scores = []\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: break\n",
    "\n",
    "            sentence_A=line.split('\\t')[1]\n",
    "            sentence_B=line.split('\\t')[2]\n",
    "            relatedness_score=line.split('\\t')[3]    \n",
    "            \n",
    "            words = sentence_A.split()\n",
    "            sentencesA_length.append(len(words))\n",
    "            sentences_A.append(words)\n",
    "            \n",
    "            words = sentence_B.split()\n",
    "            sentencesB_length.append(len(words))\n",
    "            sentences_B.append(words)\n",
    "            \n",
    "            relatedness_scores.append(((float(relatedness_score) - 1) / 4 )) # convert scores to [0,1] values\n",
    "    assert len(sentences_A)==len(sentencesA_length)==len(sentences_B)==len(sentencesB_length)==len(relatedness_scores)\n",
    "    return Input(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "\n",
    "\n",
    "def generate_word2id_dictionary(texts, min_freq=-1, insert_words=None, lowercase=False, replace_digits=False):\n",
    "    counter = collections.Counter()\n",
    "    for text in texts:\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        if replace_digits:\n",
    "            text = re.sub(r'\\d', '0', text)\n",
    "        counter.update(text.strip().split())\n",
    "\n",
    "    word2id = collections.OrderedDict()\n",
    "    if insert_words is not None:\n",
    "        for word in insert_words:\n",
    "            word2id[word] = len(word2id)\n",
    "    word_count_list = counter.most_common()\n",
    "\n",
    "    for (word, count) in word_count_list:\n",
    "        if min_freq <= 0 or count >= min_freq:\n",
    "            word2id[word] = len(word2id)\n",
    "\n",
    "    return word2id\n",
    "\n",
    "# Create an OrderedDict of words and characters and their ids based on their frequency\n",
    "dataset = read_input_file(FLAGS.all_data)\n",
    "sentences = dataset.sentences_A + dataset.sentences_B\n",
    "word2id = generate_word2id_dictionary([\" \".join(sentence) for sentence in sentences], \n",
    "                                        int(FLAGS.min_word_freq), \n",
    "                                        insert_words=[\"<unk>\"], \n",
    "                                        lowercase=False, \n",
    "                                        replace_digits=False)\n",
    "char2id = generate_word2id_dictionary([\" \".join([\" \".join(list(word)) for word in sentence]) for sentence in sentences], \n",
    "                                        min_freq=-1, \n",
    "                                        insert_words=[\"<cunk>\"], \n",
    "                                        lowercase=False, \n",
    "                                        replace_digits=False)\n",
    "\n",
    "\n",
    "\n",
    "## vocab size\n",
    "word_vocab_size = len(word2id)\n",
    "\n",
    "char_vocab_size = len(char2id)\n",
    "print(word_vocab_size, char_vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Given the word2id, load its pretrained vectors into memory\n",
    "def preload_vectors(word2vec_path, word2id, vocab_size, emb_dim):\n",
    "    if word2vec_path:\n",
    "        print('Load word2vec_norm file {}'.format(word2vec_path))\n",
    "        with open(word2vec_path,'r') as f:\n",
    "            header=f.readline()\n",
    "            print(vocab_size, emb_dim)\n",
    "            scale = np.sqrt(3.0 / emb_dim)\n",
    "            init_W = np.random.uniform(-scale, scale, [vocab_size, emb_dim])\n",
    "            \n",
    "            print('vocab_size={}'.format(vocab_size))\n",
    "            while True:\n",
    "                line=f.readline()\n",
    "                if not line:break\n",
    "                word=line.split()[0]\n",
    "                if word in word2id:\n",
    "                    init_W[word2id[word]] = np.array(line.split()[1:], dtype = \"float32\")\n",
    "    return init_W\n",
    "\n",
    "init_W = preload_vectors(FLAGS.word2vec_path, word2id, word_vocab_size, FLAGS.word_embedding_dim)\n",
    "\n",
    "print(\"preloaded vectors\")\n",
    "\n",
    "\n",
    "def map_text_to_ids(text, word2id, start_token=None, end_token=None, unk_token=None, lowercase=False, replace_digits=False):\n",
    "    ids = []\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if replace_digits:\n",
    "        text = re.sub(r'\\d', '0', text)\n",
    "\n",
    "    if start_token != None:\n",
    "        text = start_token + \" \" + text\n",
    "    if end_token != None:\n",
    "        text = text + \" \" + end_token\n",
    "    for word in text.strip().split():\n",
    "        if word in word2id:\n",
    "            ids.append(word2id[word])\n",
    "        elif unk_token != None:\n",
    "            ids.append(word2id[unk_token])\n",
    "    return ids\n",
    "\n",
    "\n",
    "def read_dataset(filename, lowercase_words, lowercase_chars, replace_digits, word2id, char2id):\n",
    "    dataset = []\n",
    "    data = read_input_file(filename)\n",
    "    sentences = [data.sentences_A, data.sentences_B]\n",
    "    max_sentence_len = 0\n",
    "    max_word_len = 0\n",
    "    for i in range(len(data.sentences_A)):\n",
    "\n",
    "        # map text to ids\n",
    "        senA_word_ids = map_text_to_ids(\" \".join(sentences[0][i]), word2id, None, None, \"<unk>\", lowercase=False, replace_digits=False)\n",
    "        senA_char_ids = [map_text_to_ids(\" \".join(list(word)), char2id, None, None, \"<cunk>\", lowercase=False, replace_digits=False) for word in sentences[0][i]]\n",
    "\n",
    "        senB_word_ids = map_text_to_ids(\" \".join(sentences[1][i]), word2id, None, None, \"<unk>\", lowercase=False, replace_digits=False)\n",
    "        senB_char_ids = [map_text_to_ids(\" \".join(list(word)), char2id, None, None, \"<cunk>\", lowercase=False, replace_digits=False) for word in sentences[1][i]]\n",
    "\n",
    "        assert(len(senA_word_ids) == len(senA_char_ids))\n",
    "        assert(len(senB_word_ids) == len(senB_char_ids))\n",
    "        \n",
    "        \n",
    "        senA_len = len(senA_word_ids)\n",
    "        senA_words_len = [len(word) for word in senA_char_ids]\n",
    "        senB_len = len(senB_word_ids)\n",
    "        senB_words_len = [len(word) for word in senB_char_ids]\n",
    "        \n",
    "        max_sentence_len = max(max_sentence_len, senA_len, senB_len)\n",
    "        max_word_len = max(max(senA_words_len), max(senB_words_len), max_word_len)\n",
    "        \n",
    "        dataset.append((senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, data.relatedness_scores[i]))\n",
    "    return dataset, max_sentence_len, max_word_len\n",
    "\n",
    "data_train, max_sentence_len_train, max_word_len_train = read_dataset(FLAGS.train_data, False, False, False, word2id, char2id)\n",
    "data_dev, max_sentence_len_dev, max_word_len_dev = read_dataset(FLAGS.validation_data, False, False, False, word2id, char2id)\n",
    "data_test, max_sentence_len_test, max_word_len_test = read_dataset(FLAGS.test_data, False, False, False, word2id, char2id)\n",
    "print(\"dataset read\")\n",
    "\n",
    "'''\n",
    "Get the max_sentence_len and max_word_len\n",
    "'''\n",
    "## Check the sentence\n",
    "# max_sentence_len = sorted(([(len(sentence), sentence) for sentence in sentences]))[-1]\n",
    "max_sentence_len = max(max_sentence_len_train, max_sentence_len_test, max_sentence_len_dev)\n",
    "print(max_sentence_len)\n",
    "\n",
    "## Check the word\n",
    "# max_word_len = sorted(([(len(word), word) for word in word2id]))[-1]\n",
    "max_word_len = max(max_word_len_train, max_word_len_test, max_word_len_dev)\n",
    "print(max_word_len)\n",
    "\n",
    "\n",
    "def pad_to_max_len_sentence_and_word_len(data, max_sentence_len, max_word_len):\n",
    "    dataset = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        senA_word_ids = data[i][0]\n",
    "        senB_word_ids = data[i][4]\n",
    "        senA_char_ids = data[i][2]\n",
    "        senB_char_ids = data[i][6]\n",
    "        senA_words_len = data[i][3]\n",
    "        senB_words_len = data[i][7]\n",
    "        senA_len = data[i][1]\n",
    "        senB_len = data[i][5]\n",
    "        relatedness_scores = data[i][8]\n",
    "        \n",
    "        # pad values to max_sentence_len \n",
    "        senA_word_ids += [0] * (max_sentence_len - len(senA_word_ids))\n",
    "        senB_word_ids += [0] * (max_sentence_len - len(senB_word_ids))\n",
    "               \n",
    "        assert(len(senA_word_ids) == len(senB_word_ids))\n",
    "        \n",
    "        # pad values to max_word_len and pad each sentence to max_sentence_len\n",
    "        senA_char_ids_wl = [word + ([0] * (max_word_len - len(word))) for word in senA_char_ids]\n",
    "        senA_char_ids_sl = [([0] * (max_word_len))] * (max_sentence_len - len(senA_char_ids_wl))\n",
    "        senA_char_ids = senA_char_ids_wl + senA_char_ids_sl\n",
    "\n",
    "        senB_char_ids_wl = [word + ([0] * (max_word_len - len(word))) for word in senB_char_ids]\n",
    "        senB_char_ids_sl = [([0] * (max_word_len))] * (max_sentence_len - len(senB_char_ids_wl))\n",
    "        senB_char_ids = senB_char_ids_wl + senB_char_ids_sl\n",
    "\n",
    "        # pad word lengths to max_sentence_len\n",
    "        senA_words_len = senA_words_len + ([0] * (max_sentence_len - len(senA_words_len)))\n",
    "        senB_words_len = senB_words_len + ([0] * (max_sentence_len - len(senB_words_len)))\n",
    "        \n",
    "        dataset.append((senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, relatedness_scores))\n",
    "    return dataset\n",
    "\n",
    "data_train = pad_to_max_len_sentence_and_word_len(data_train, max_sentence_len, max_word_len)\n",
    "data_dev = pad_to_max_len_sentence_and_word_len(data_dev, max_sentence_len, max_word_len)\n",
    "data_test = pad_to_max_len_sentence_and_word_len(data_test, max_sentence_len, max_word_len)\n",
    "\n",
    "zipped_train = zip(*data_train)\n",
    "zipped_dev = zip(*data_dev)\n",
    "zipped_test = zip(*data_test)\n",
    "len(zipped_train[3][0])\n",
    "\n",
    "print(\"pad_to_max_len_sentence_and_word_len\")\n",
    "\n",
    "\n",
    "def next_batch(start,end,input):\n",
    "    senA_word_ids = input[0][start:end]\n",
    "    senA_len = input[1][start:end]\n",
    "    senA_char_ids = input[2][start:end]\n",
    "    senA_words_len = input[3][start:end]\n",
    "    senB_word_ids = input[4][start:end]\n",
    "    senB_len = input[5][start:end]\n",
    "    senB_char_ids = input[6][start:end]\n",
    "    senB_words_len = input[7][start:end]\n",
    "    relatedness_scores = np.reshape(input[8][start:end],(-1))\n",
    "    return Batch(senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, relatedness_scores)\n",
    "\n",
    "\n",
    "class Batch(object):\n",
    "    def __init__(self,senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, relatedness_scores):\n",
    "        self.senA_word_ids = senA_word_ids\n",
    "        self.senA_len = senA_len\n",
    "        self.senA_char_ids = senA_char_ids\n",
    "        self.senA_words_len = senA_words_len\n",
    "        self.senB_word_ids = senB_word_ids\n",
    "        self.senB_len = senB_len\n",
    "        self.senB_char_ids = senB_char_ids\n",
    "        self.senB_words_len = senB_words_len\n",
    "        self.relatedness_scores = relatedness_scores\n",
    "    def senA_word_ids(self):\n",
    "        return self.senA_word_ids\n",
    "    \n",
    "    def senA_len(self):\n",
    "        return self.senA_len\n",
    "    \n",
    "    def senA_char_ids(self):\n",
    "        return self.senA_char_ids\n",
    "    \n",
    "    def senA_words_len(self):\n",
    "        return self.senA_words_len\n",
    "    \n",
    "    def senB_word_ids(self):\n",
    "        return self.senB_word_ids\n",
    "    \n",
    "    def senB_len(self):\n",
    "        return self.senB_len\n",
    "    \n",
    "    def senB_char_ids(self):\n",
    "        return self.senB_char_ids\n",
    "    \n",
    "    def senB_words_len(self):\n",
    "        return self.senB_words_len\n",
    "    \n",
    "    def relatedness_scores(self):\n",
    "        return self.relatedness_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_model(input_,input_length,dropout_):##should try this\n",
    "    rnn_cell=tf.nn.rnn_cell.LSTMCell(config.num_units)##why 50?\n",
    "    rnn_cell=tf.nn.rnn_cell.DropoutWrapper(rnn_cell,output_keep_prob=dropout_)\n",
    "    rnn_cell=tf.nn.rnn_cell.MultiRNNCell([rnn_cell]*config.num_layer)\n",
    "        \n",
    "    outputs,last_states=tf.nn.dynamic_rnn(\n",
    "        cell=rnn_cell,\n",
    "        dtype=data_type(),\n",
    "        sequence_length=input_length,\n",
    "        inputs=input_\n",
    "    )\n",
    "    return outputs,last_states\n",
    "\n",
    "print(\"build_model\")\n",
    "print(\"training..\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>: Model/char_embeddings:0\n",
      ">>: Model/siamese_char/RNN/MultiRNNCell/Cell0/LSTMCell/W_0:0\n",
      ">>: Model/siamese_char/RNN/MultiRNNCell/Cell0/LSTMCell/B:0\n",
      ">>: Model/siamese_char/siamese_char_ff_a/C_ff_weights_a:0\n",
      ">>: Model/siamese_char/siamese_char_ff_a/C_ff_bias_a:0\n",
      ">>: Model/siamese_char/siamese_char_ff_b/C_ff_weights_b:0\n",
      ">>: Model/siamese_char/siamese_char_ff_b/C_ff_bias_b:0\n",
      "Total batch size: 116, data size: 3499, batch size: 30\n",
      "1.0 0.5 0.98 8 100 1\n",
      "Epoch 0 Learning rate: 0.00999999977648\n",
      "i: 0\n",
      "c_outputs_A,c_last_states_A (960, 16, 50) (960, 50)\n",
      "0.0\n",
      "-0.0253704\n",
      "i: 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument array([[[-0.05074088, -0.11606301,  0.        , ..., -0.17377019,\n         -0.14895239,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        ..., \n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[-0.04078784, -0.        ,  0.        , ..., -0.10446665,\n         -0.        ,  0.09874545],\n        [-0.15369676,  0.00976905,  0.13587974, ..., -0.24608652,\n         -0.16111068,  0.        ],\n        [ 0.09228917,  0.34844717, -0.13119029, ..., -0.21002316,\n          0.        ,  0.        ],\n        ..., \n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[-0.        ,  0.        ,  0.04235517, ..., -0.        ,\n         -0.        ,  0.        ],\n        [ 0.0985599 ,  0.03913999, -0.0423911 , ..., -0.        ,\n          0.02988634,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        ..., \n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       ..., \n       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        ..., \n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        ..., \n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        ..., \n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]]], dtype=float32) has invalid type <type 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ce6f1eab7324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m                             \u001b[0msentencesB_words_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext_batch_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenB_words_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                             \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext_batch_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelatedness_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                             \u001b[0mdropout_f\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;31m#                             embedding_placeholder: init_W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         })\n",
      "\u001b[0;32m/Users/kurt/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kurt/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m     \u001b[0mfetch_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kurt/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \"\"\"\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kurt/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kurt/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \"\"\"\n\u001b[1;32m    336\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kurt/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[0;32m/Users/kurt/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    269\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[1;32m    270\u001b[0m                         \u001b[0;34m'must be a string or Tensor. (%s)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                         % (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[1;32m    272\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument array([[[-0.05074088, -0.11606301,  0.        , ..., -0.17377019,\n         -0.14895239,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        ..., \n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[-0.04078784, -0.        ,  0.        , ..., -0.10446665,\n         -0.        ,  0.09874545],\n        [-0.15369676,  0.00976905,  0.13587974, ..., -0.24608652,\n         -0.16111068,  0.        ],\n        [ 0.09228917,  0.34844717, -0.13119029, ..., -0.21002316,\n          0.        ,  0.        ],\n        ..., \n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[-0.        ,  0.        ,  0.04235517, ..., -0.        ,\n         -0.        ,  0.        ],\n        [ 0.0985599 ,  0.03913999, -0.0423911 , ..., -0.        ,\n          0.02988634,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        ..., \n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       ..., \n       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        ..., \n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        ..., \n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        ..., \n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]]], dtype=float32) has invalid type <type 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    initializer=tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "    with tf.variable_scope('Model',initializer=initializer):\n",
    "        \n",
    "#         # word embedding\n",
    "#         sentences_A = tf.placeholder(tf.int32, shape = ([None, max_sentence_len]), name='sentences_A')\n",
    "#         sentencesA_length = tf.placeholder(tf.int32, shape=([None]),name='sentencesA_length')\n",
    "#         sentences_B = tf.placeholder(tf.int32, shape = ([None, max_sentence_len]), name='sentences_B')\n",
    "#         sentencesB_length = tf.placeholder(tf.int32, shape=([None]), name='sentencesB_length')\n",
    "        labels = tf.placeholder(tf.float32, shape=([None]),name='relatedness_score_label')\n",
    "        dropout_f = tf.placeholder(tf.float32)\n",
    "\n",
    "#         # fine-tune by setting trainable to True\n",
    "#         W = tf.Variable(tf.constant(0.0, shape = [word_vocab_size,FLAGS.word_embedding_dim]),trainable = True, name='word_embeddings')\n",
    "#         embedding_placeholder = tf.placeholder(tf.float32, shape = [word_vocab_size, FLAGS.word_embedding_dim])\n",
    "#         embedding_init = W.assign(embedding_placeholder)\n",
    "\n",
    "#         # sentences_A_word_emb (30, 32, 300)\n",
    "#         sentences_A_word_emb = tf.nn.embedding_lookup(params=embedding_init,ids=sentences_A)\n",
    "#         sentences_B_word_emb = tf.nn.embedding_lookup(params=embedding_init,ids=sentences_B)\n",
    "        \n",
    "\n",
    "        #character embedding\n",
    "        # C (131, 50)\n",
    "        C = tf.Variable(tf.random_uniform([char_vocab_size, FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"char_embeddings\")\n",
    "        # sentences_words_A (30, 32, 16)\n",
    "        sentences_words_A = tf.placeholder(tf.int32, shape = ([None, max_sentence_len, max_word_len]), name='sentences_words_A')\n",
    "        # sentencesA_words_length (30, 32)\n",
    "        sentencesA_words_length = tf.placeholder(tf.int32, shape=([None, max_sentence_len]),name='sentencesA_words_length')    \n",
    "        \n",
    "        sentences_words_B = tf.placeholder(tf.int32, shape = ([None, max_sentence_len, max_word_len]), name='sentences_words_B')\n",
    "        sentencesB_words_length = tf.placeholder(tf.int32, shape=([None, max_sentence_len]),name='sentencesB_words_length')    \n",
    "        \n",
    "        # sentences_A_char_emb (30, 32, 16, 50)\n",
    "        sentences_A_char_emb = tf.nn.embedding_lookup(params = C, ids = sentences_words_A)\n",
    "        sentences_B_char_emb = tf.nn.embedding_lookup(params = C, ids = sentences_words_B)\n",
    "            \n",
    "        #reshape char embedding\n",
    "        # sentences_A_char_emb_1 (960, 16, 50)\n",
    "        sentences_A_char_emb_1 = tf.reshape(sentences_A_char_emb, shape = ([-1, max_word_len, FLAGS.char_embedding_dim]))\n",
    "        # sentencesA_words_length_1 (960,)\n",
    "        sentencesA_words_length_1 = tf.reshape(sentencesA_words_length, shape = ([-1]))\n",
    "        sentences_B_char_emb_1 = tf.reshape(sentences_B_char_emb, shape = ([-1, max_word_len, FLAGS.char_embedding_dim]))\n",
    "        sentencesB_words_length_1 = tf.reshape(sentencesB_words_length, shape = ([-1]))\n",
    "\n",
    "        with tf.variable_scope('siamese_char') as scope:\n",
    "            # feed to biLSTM\n",
    "            c_outputs_A,c_last_states_A = build_model(sentences_A_char_emb_1,sentencesA_words_length_1,dropout_f)\n",
    "            scope.reuse_variables()\n",
    "            c_outputs_B,c_last_states_B = build_model(sentences_B_char_emb_1,sentencesB_words_length_1,dropout_f)\n",
    "\n",
    "#             c_output_state_fw_A, c_output_state_bw_A = c_last_states_A\n",
    "\n",
    "            # Each of the two tuples above has c and h, cell state and output respectively.\n",
    "#             c_last_states_A_ct = tf.concat(2, [c_output_state_fw_A[0], c_output_state_bw_A[0]])\n",
    "            # we use [0] because are interested with the c.\n",
    "#             c_last_states_A_ct_ff = tf.reshape(c_last_states_A_ct[0], shape = ([-1, config.num_units * 2]))\n",
    "            # feed to feedforward layer hidden states of sentences A\n",
    "            with tf.variable_scope('siamese_char_ff_a') as scope:\n",
    "                # 50 x 50\n",
    "                C_ff_weights_a = tf.Variable(tf.random_uniform([config.num_units, FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"C_ff_weights_a\")\n",
    "                # 50\n",
    "                C_ff_bias_a = tf.Variable(tf.random_uniform([FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"C_ff_bias_a\")\n",
    "                # 960 x 50 * 50 x 50 \n",
    "                c_output_a = tf.add(tf.matmul(c_last_states_A[0].h, C_ff_weights_a), C_ff_bias_a)\n",
    "                # 960 x 16 x 50\n",
    "                c_output_a = tf.tanh(c_output_a)\n",
    "                # ? x 32 x 16 x 50\n",
    "                c_output_a = tf.reshape(c_output_a, shape = ([-1, max_sentence_len, FLAGS.char_embedding_dim]))\n",
    "            \n",
    "#             c_output_state_fw_B, c_output_state_bw_B = c_last_states_B\n",
    "            # TODO: Why [0] and [1]??\n",
    "#             c_last_states_B_ct = tf.concat(2, [c_output_state_fw_B[0], c_output_state_bw_B[0]])\n",
    "#             c_last_states_B_ct_ff = tf.reshape(c_last_states_B_ct[0], shape = ([-1, config.num_units * 2]))\n",
    "\n",
    "#             # feed to feedforward layer hidden states of sentences B\n",
    "            with tf.variable_scope('siamese_char_ff_b') as scope:\n",
    "                C_ff_weights_b = tf.Variable(tf.random_uniform([config.num_units, FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"C_ff_weights_b\")\n",
    "                C_ff_bias_b = tf.Variable(tf.random_uniform([FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"C_ff_bias_b\")\n",
    "                c_output_b = tf.add(tf.matmul(c_last_states_B[0].h, C_ff_weights_b), C_ff_bias_b)\n",
    "                c_output_b = tf.tanh(c_output_b)\n",
    "                c_output_b = tf.reshape(c_output_b, shape = ([-1, max_sentence_len, FLAGS.char_embedding_dim]))\n",
    "        \n",
    "        # concatenate char + word embeddings here\n",
    "        # TODO: what does concat mean here? add embeddings element-wise or concat them but on which axis?\n",
    "#         final_emb_A = tf.add(sentences_A_word_emb, c_output_a)\n",
    "#         final_emb_B = tf.add(sentences_B_word_emb, c_output_b)\n",
    "        # final_emb_A = tf.concat(2, [sentences_A_word_emb, c_output_a])\n",
    "        # final_emb_B = tf.concat(2, [sentences_B_word_emb, c_output_b])\n",
    "\n",
    "#         final_emb_A = c_output_a\n",
    "#         final_emb_B = c_output_b\n",
    "#         seq_len_A = sentencesA_words_length\n",
    "#         seq_len_B = sentencesB_words_length\n",
    "\n",
    "        # Feed the concatenation of embeddings to biLSTM\n",
    "#         with tf.variable_scope('siamese_final') as scope:\n",
    "#             outputs_A,last_states_A = build_model(final_emb_A,seq_len_A,dropout_f)\n",
    "#             scope.reuse_variables()\n",
    "#             outputs_B,last_states_B = build_model(final_emb_B,seq_len_B,dropout_f)\n",
    "\n",
    "#             # last_states_A_bw (30, 50)\n",
    "#             last_states_A_fw, last_states_A_bw =  last_states_A\n",
    "#             # last_states_A_ct (2, 30, 100)\n",
    "#             last_states_A_ct = tf.concat(2, [last_states_A_fw[0], last_states_A_bw[0]])\n",
    "#             # last_states_A_ct_c (30, 100)\n",
    "#             last_states_A_ct_c = last_states_A_ct[0]\n",
    "\n",
    "#             last_states_B_fw, last_states_B_bw =  last_states_B\n",
    "#             last_states_B_ct = tf.concat(2, [last_states_B_fw[0], last_states_B_bw[0]])\n",
    "#             last_states_B_ct_c = last_states_B_ct[0]            \n",
    "\n",
    "#         # Compare last hidden states of both batch of sentences\n",
    "#         # TODO: make sense of [0] \n",
    "        prediction = tf.exp(tf.mul(-1.0,tf.reduce_mean(tf.reduce_mean(tf.abs(tf.sub(c_output_b,c_output_a)),1),1)))\n",
    "        cost = tf.reduce_mean(tf.square(tf.sub(prediction, labels)))\n",
    "\n",
    "        lr = tf.Variable(0.0,trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads,_ = tf.clip_by_global_norm(tf.gradients(cost,tvars),config.max_grad_norm)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        train_op = optimizer.apply_gradients(zip(grads,tvars),global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        new_lr = tf.placeholder(tf.float32,shape=[],name='new_learning_rate')\n",
    "        lr_update = tf.assign(lr,new_lr)\n",
    "        \n",
    "        for v in tf.trainable_variables():\n",
    "            print(\">>:\", v.name)\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        ## Launch training graph\n",
    "        with tf.Session(config=config_gpu) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            total_batch = int(len(zipped_train[0]) / config.batch_size)##this doesn't include the extra samples? It does. see below after the \"if\" block.\n",
    "            print('Total batch size: {}, data size: {}, batch size: {}'.format(total_batch,len(zipped_train[0]),config.batch_size))\n",
    "            print(config.max_grad_norm,config.keep_prob,config.lr_decay,config.lr_max_epoch,config.train_max_epoch,config.num_layer)\n",
    "            # train\n",
    "            prev_train_cost=1\n",
    "            prev_valid_cost=1\n",
    "            \n",
    "            patience = config.patience\n",
    "            train_costs = []\n",
    "            valid_costs = []\n",
    "            \n",
    "            \n",
    "            for epoch in range(config.train_max_epoch):\n",
    "                lr_decay = config.lr_decay**max(epoch+1-config.lr_max_epoch,0.0)\n",
    "                sess.run([lr,lr_update],feed_dict = {new_lr:config.learning_rate*lr_decay})\n",
    "                \n",
    "                print('Epoch {} Learning rate: {}'.format(epoch,sess.run(lr)))\n",
    "                \n",
    "                avg_cost=0.\n",
    "                \n",
    "                for i in range(total_batch):\n",
    "                    print(\"i:\", i)\n",
    "                    start = i*config.batch_size\n",
    "                    end = (i+1)*config.batch_size\n",
    "                    next_batch_input = next_batch(start,end,zipped_train)\n",
    "\n",
    "                    _,train_cost, c_outputs_A,c_last_states_A = sess.run([train_op,cost, c_outputs_A,c_last_states_A], feed_dict={   \n",
    "#                     cost,prediction,c_output_b, c_output_a= sess.run([cost,prediction,c_output_b, c_output_a], feed_dict={   \n",
    "#                     c_output_a,c_output_a_t, \n",
    "#                     c_output_a,c_output_a_t,c_output_a_m, C_ff_bias_a, C_ff_weights_a,sentences_A_char_emb_1, sentencesA_words_length_1,c_last_states_A= sess.run([sentences_A_char_emb_1, sentencesA_words_length_1,c_last_states_A,c_output_a_m, C_ff_bias_a, C_ff_weights_a,c_output_a_t, c_output_a], feed_dict={   \n",
    "#                     last_states_A_ct_c, sentencesA_words_length, sentencesA_words_length_1, C, sentences_A_char_emb_1, train_cost,  prediction, last_states_A_ct, last_states_A_bw, final_emb_A, c_last_states_A_ct_ff, c_last_states_A_ct, c_output_state_fw_A, sentences_A_char_emb,sentences_words_A, sentences_A_word_emb= sess.run([last_states_A_ct_c, sentencesA_words_length, sentencesA_words_length_1, C, sentences_A_char_emb_1, cost, prediction,  last_states_A_ct, last_states_A_bw, final_emb_A, c_last_states_A_ct_ff, c_last_states_A_ct, c_output_state_fw_A, sentences_A_char_emb,sentences_words_A, sentences_A_word_emb], feed_dict={                                         \n",
    "#                             sentences_A: next_batch_input.senA_word_ids,\n",
    "#                             sentencesA_length: next_batch_input.senA_len,\n",
    "#                             sentences_B: next_batch_input.senB_word_ids,\n",
    "#                             sentencesB_length: next_batch_input.senB_len,\n",
    "                            \n",
    "                            sentences_words_A: next_batch_input.senA_char_ids,\n",
    "                            sentencesA_words_length: next_batch_input.senA_words_len,\n",
    "                            sentences_words_B: next_batch_input.senB_char_ids,\n",
    "                            sentencesB_words_length: next_batch_input.senB_words_len,\n",
    "                            labels: next_batch_input.relatedness_scores,\n",
    "                            dropout_f: config.keep_prob\n",
    "#                             embedding_placeholder: init_W\n",
    "                        })\n",
    "                    avg_cost += train_cost\n",
    "            \n",
    "                    print(\"c_outputs_A,c_last_states_A\", c_outputs_A.shape,c_last_states_A[0].h.shape)\n",
    "                    print(c_outputs_A[0][15])\n",
    "                    print(c_last_states_A[0].h[0][0])\n",
    "#                     print(\"sentences_A_char_emb_1\", sentences_A_char_emb_1.shape)\n",
    "#                     print(\"sentencesA_words_length_1\", sentencesA_words_length_1.shape)\n",
    "#                     print(\"c_last_states_A\", len(c_last_states_A))\n",
    "# #                     print(\"c_last_states_A\", c_last_states_A[0].c.shape)\n",
    "# #                     print(\"C_ff_bias_a\", C_ff_bias_a[0].c.shape)\n",
    "# #                     print(\"c_output_a_m\", c_output_a_m.shape)\n",
    "#                     print(\"c_output_a_t\", c_output_a_t.shape)\n",
    "#                     print(\"c_output_a\", c_output_a.shape)\n",
    "#                     print(\"c_output_b\", c_output_b.shape)\n",
    "#                     print(\"prediction\", prediction.shape)\n",
    "#                     print(\"cost\", cost)\n",
    "#                     break\n",
    "                    \n",
    "                start = total_batch*config.batch_size\n",
    "                end = len(zipped_train[0])\n",
    "                #check if the last trailing batch and handle it\n",
    "                if not start == end:\n",
    "                    next_batch_input = next_batch(start,end,zipped_train)\n",
    "                    _,train_cost= sess.run([train_op,cost], feed_dict={                                         \n",
    "#                             sentences_A: next_batch_input.senA_word_ids,\n",
    "#                             sentencesA_length: next_batch_input.senA_len,\n",
    "#                             sentences_B: next_batch_input.senB_word_ids,\n",
    "#                             sentencesB_length: next_batch_input.senB_len,\n",
    "                            \n",
    "                            sentences_words_A: next_batch_input.senA_char_ids,\n",
    "                            sentencesA_words_length: next_batch_input.senA_words_len,\n",
    "                            sentences_words_B: next_batch_input.senB_char_ids,\n",
    "                            sentencesB_words_length: next_batch_input.senB_words_len,\n",
    "                            labels: next_batch_input.relatedness_scores,\n",
    "                            dropout_f: config.keep_prob\n",
    "#                             embedding_placeholder: init_W\n",
    "                        })\n",
    "                    avg_cost += train_cost\n",
    "                \n",
    "                if prev_train_cost >  avg_cost / total_batch: \n",
    "                    print('Average cost:\\t{} >'.format(avg_cost / total_batch))\n",
    "                else: \n",
    "                    print('Average cost:\\t{} <'.format(avg_cost / total_batch))\n",
    "\n",
    "                prev_train_cost = avg_cost / total_batch\n",
    "                \n",
    "                train_costs.append(avg_cost)\n",
    "\n",
    "                \n",
    "                # validation\n",
    "                next_batch_input = next_batch(0, len(zipped_dev[0]), zipped_dev)\n",
    "                valid_cost,valid_predict=sess.run([cost,prediction],feed_dict={\n",
    "#                         sentences_A: next_batch_input.senA_word_ids,\n",
    "#                         sentencesA_length: next_batch_input.senA_len,\n",
    "#                         sentences_B: next_batch_input.senB_word_ids,\n",
    "#                         sentencesB_length: next_batch_input.senB_len,\n",
    "\n",
    "                        sentences_words_A: next_batch_input.senA_char_ids,\n",
    "                        sentencesA_words_length: next_batch_input.senA_words_len,\n",
    "                        sentences_words_B: next_batch_input.senB_char_ids,\n",
    "                        sentencesB_words_length: next_batch_input.senB_words_len,\n",
    "                        labels: next_batch_input.relatedness_scores,\n",
    "                        dropout_f: config.keep_prob#,\n",
    "#                         embedding_placeholder: init_W\n",
    "                })\n",
    "                if prev_valid_cost > valid_cost: \n",
    "                    print('Valid cost:\\t{} >'.format(valid_cost))\n",
    "                else: \n",
    "                    print('Valid cost:\\t{} <'.format(valid_cost))\n",
    "                prev_valid_cost=valid_cost\n",
    "                \n",
    "                valid_costs.append(valid_cost)\n",
    "                \n",
    "                # early stopping\n",
    "                if patience == 0:\n",
    "                    print(\"Lost patience:\", patience)\n",
    "                    break\n",
    "                if prev_valid_cost > valid_cost:\n",
    "                    patience -= 1\n",
    "                else:\n",
    "                    patience = 5\n",
    "                print(\"patience:\", patience)\n",
    "#                 break\n",
    "            saver.save(sess, FLAGS.save_path+FLAGS.model_name,global_step=config.train_max_epoch)\n",
    "\n",
    "            # test\n",
    "            next_batch_input = next_batch(0, len(zipped_test[0]), zipped_test)\n",
    "            test_cost,test_predict=sess.run([cost,prediction],feed_dict={\n",
    "#                     sentences_A: next_batch_input.senA_word_ids,\n",
    "#                     sentencesA_length: next_batch_input.senA_len,\n",
    "#                     sentences_B: next_batch_input.senB_word_ids,\n",
    "#                     sentencesB_length: next_batch_input.senB_len,\n",
    "\n",
    "                    sentences_words_A: next_batch_input.senA_char_ids,\n",
    "                    sentencesA_words_length: next_batch_input.senA_words_len,\n",
    "                    sentences_words_B: next_batch_input.senB_char_ids,\n",
    "                    sentencesB_words_length: next_batch_input.senB_words_len,\n",
    "                    labels: next_batch_input.relatedness_scores,\n",
    "                    dropout_f: config.keep_prob#,\n",
    "#                     embedding_placeholder: init_W         \n",
    "            })\n",
    "            print(test_cost)\n",
    "\n",
    "\n",
    "            \n",
    "with open(FLAGS.result_path + FLAGS.model_name+'.txt','w') as fw:\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for _ in range(len(test_predict)):\n",
    "        fw.write(str(next_batch_input.relatedness_scores[_])+'\\t'+str(test_predict[_])+'\\n')\n",
    "        labels.append(next_batch_input.relatedness_scores[_])\n",
    "        preds.append(test_predict[_])\n",
    "    print(\"Pearson:\", pearsonr(labels,preds))\n",
    "stop = timeit.default_timer()\n",
    "print(\"Execution time:\", stop - start)\n",
    "\n",
    "\n",
    "y1 = train_costs\n",
    "y2 = valid_costs\n",
    "x_axis = range(1, len(y1)+1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis, y1, label='Train Cost')\n",
    "ax.plot(x_axis, y2, label='Validation Cost')\n",
    "ax.legend()\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training vs Validation Cost')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.savefig(FLAGS.result_path + FLAGS.model_name+\".png\")\n",
    "\n",
    "test_data = read_input_file(FLAGS.test_data)\n",
    "cnt = int(FLAGS.cnt)\n",
    "with open(FLAGS.result_path + FLAGS.model_name+'.txt', 'r') as f:\n",
    "    a = []\n",
    "    b = []\n",
    "    for line in f:\n",
    "        a.append(float(line.strip().split('\\t')[0]))\n",
    "        b.append(float(line.strip().split('\\t')[1]))\n",
    "    #most dissimilar/similar actual/predictions values\n",
    "    res = [abs(a[i] - b[i]) for i in range(len(a))]\n",
    "    sort = sorted(range(len(res)), key=lambda k: res[k], reverse = True)\n",
    "    firstn = sort[0:cnt]\n",
    "    lastn = sort[-(cnt):len(sort)]\n",
    "    lastn = lastn[::-1]\n",
    "    \n",
    "    #prediction scores\n",
    "    predscores = sorted(range(len(b)), key=lambda k: b[k], reverse = True)\n",
    "    highpreds = predscores[0:cnt]\n",
    "    lowpreds = predscores[-(cnt):len(predscores)]\n",
    "    lowpreds = lowpreds[::-1]\n",
    "with open(FLAGS.result_path + FLAGS.model_name+'_samples.txt', 'w') as f:\n",
    "#     print(\"Most dissimilar (actual - predicted)\\n\")\n",
    "    f.write(\"Most dissimilar (actual - predicted)\\n\")\n",
    "    for i in range(len(test_data.sentences_A)):\n",
    "    #     print(i)\n",
    "        txt = str(next_batch_input.relatedness_scores[firstn[i]])+\"\\t\"+str(test_predict[firstn[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[firstn[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[firstn[i]]))\n",
    "#         print(txt)\n",
    "        f.write(txt+\"\\n\")\n",
    "        if i == cnt-1:\n",
    "            break\n",
    "\n",
    "\n",
    "#     print(\"Most similar (actual - predicted)\\n\")\n",
    "    f.write(\"Most similar (actual - predicted)\\n\")\n",
    "    length = len(test_data.sentences_A)\n",
    "    for i in range(length):\n",
    "    #     print(i)\n",
    "        txt = str(next_batch_input.relatedness_scores[lastn[i]])+\"\\t\"+str(test_predict[lastn[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[lastn[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[lastn[i]]))\n",
    "#         print(txt)\n",
    "        f.write(txt+\"\\n\")\n",
    "        if i == cnt-1:\n",
    "            break\n",
    "#     print(\"Most dissimilar sentence pairs as predicted\\n\")\n",
    "    f.write(\"Most dissimilar sentence pairs as predicted\\n\")\n",
    "    for i in range(len(test_data.sentences_A)):\n",
    "    #     print(i)\n",
    "        txt = str(next_batch_input.relatedness_scores[lowpreds[i]])+\"\\t\"+str(test_predict[lowpreds[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[lowpreds[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[lowpreds[i]]))\n",
    "#         print(txt)\n",
    "        f.write(txt+\"\\n\")\n",
    "        if i == cnt-1:\n",
    "            break\n",
    "\n",
    "\n",
    "#     print(\"Most similar sentence pairs as predicted\\n\")\n",
    "    f.write(\"Most similar sentence pairs as predicted\\n\")\n",
    "    length = len(test_data.sentences_A)\n",
    "    for i in range(length):\n",
    "    #     print(i)\n",
    "        txt = str(next_batch_input.relatedness_scores[highpreds[i]])+\"\\t\"+str(test_predict[highpreds[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[highpreds[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[highpreds[i]]))\n",
    "#         print(txt)\n",
    "        f.write(txt+\"\\n\")\n",
    "        if i == cnt-1:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
