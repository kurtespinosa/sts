{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Experiment on Saliency score\n",
    "'''\n",
    "#TODO for server\n",
    "'''\n",
    "1. uncomment the matplotlib command to generate images in the server\n",
    "2. comment plt.show()\n",
    "3. change train_max_epoch\n",
    "4. update model/expt #\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import collections\n",
    "import re\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "#comment: for server only\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "\n",
    "# Model Hyperparameters\n",
    "flags=tf.flags\n",
    "\n",
    "flags.DEFINE_string('word2vec_path','embeddings/GoogleNews-vectors-negative300.txt','Word2vec file with pre-trained embeddings')\n",
    "flags.DEFINE_string('data_path','data','data set path')\n",
    "flags.DEFINE_string('min_word_freq','2','Minimum word frequency')\n",
    "flags.DEFINE_string('save_path','model/','STS model output directory')\n",
    "flags.DEFINE_string('result_path','result/','data set path')\n",
    "flags.DEFINE_string('test_data',\"data/SICK/SICK_test_annotated.txt\",'Test data')\n",
    "flags.DEFINE_string('validation_data',\"data/SICK/SICK_new_trial.txt\",'Validation data')\n",
    "flags.DEFINE_string('train_data',\"data/SICK/\",'Train data')\n",
    "flags.DEFINE_string('all_data',\"data/SICK/SICK_all.txt\",'Train data')\n",
    "flags.DEFINE_string('cnt',\"100\",'Number of samples to show')\n",
    "flags.DEFINE_string('model_name','23','Filename of the model file')\n",
    "flags.DEFINE_integer('word_embedding_dim',300,'Dimensionality of word embedding')\n",
    "flags.DEFINE_integer('char_embedding_dim',50,'Dimensionality of char embedding')\n",
    "flags.DEFINE_bool('use_fp64',False,'Train using 64-bit floats instead of 32bit floats')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "all_data=data/SICK/SICK_all.txt\n",
      "char_embedding_dim=50\n",
      "cnt=100\n",
      "data_path=data\n",
      "min_word_freq=2\n",
      "model_name=23\n",
      "result_path=result/\n",
      "save_path=model/\n",
      "test_data=data/SICK/SICK_test_annotated.txt\n",
      "train_data=data/SICK/\n",
      "use_fp64=False\n",
      "validation_data=data/SICK/SICK_new_trial.txt\n",
      "word2vec_path=embeddings/GoogleNews-vectors-negative300.txt\n",
      "word_embedding_dim=300\n"
     ]
    }
   ],
   "source": [
    "FLAGS=flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print('Parameters:')\n",
    "for attr,value in sorted(FLAGS.__flags.items()):\n",
    "    print('{}={}'.format(attr,value))\n",
    "    \n",
    "    \n",
    "\n",
    "# model parameters\n",
    "class Config(object):\n",
    "    init_scale=0.2\n",
    "    learning_rate=.01\n",
    "    max_grad_norm=1.\n",
    "    keep_prob=0.5\n",
    "    lr_decay=0.98\n",
    "    batch_size=30\n",
    "    lr_max_epoch=8##this is for learning rate epoch\n",
    "    train_max_epoch=10\n",
    "    num_layer=1\n",
    "    patience=5\n",
    "    \n",
    "config=Config()\n",
    "config_gpu = tf.ConfigProto()\n",
    "config_gpu.gpu_options.allow_growth = True\n",
    "    \n",
    "def data_type():\n",
    "    return tf.float64 if FLAGS.use_fp64 else tf.float32\n",
    "\n",
    "num_units=[50]\n",
    "training_files = ['SICK_new_train.txt']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2556 54\n",
      "Load word2vec_norm file embeddings/GoogleNews-vectors-negative300.txt\n",
      "2556 300\n",
      "vocab_size=2556\n",
      "preloaded vectors\n",
      "dataset read\n",
      "pad_to_max_len_sentence_and_word_len\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Input(object):\n",
    "    def __init__(self,sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores):\n",
    "        self.sentences_A=sentences_A\n",
    "        self.sentencesA_length=sentencesA_length\n",
    "        self.sentences_B=sentences_B\n",
    "        self.sentencesB_length=sentencesB_length\n",
    "        self.relatedness_scores=relatedness_scores\n",
    "    \n",
    "    def sentences_A(self):\n",
    "        return self.sentences_A\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesA_length\n",
    "    \n",
    "    def sentences_B(self):\n",
    "        return self.sentences_B\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesB_length\n",
    "    \n",
    "    def relatedness_scores(self):\n",
    "        return self.relatedness_scores\n",
    "\n",
    "\n",
    "'''\n",
    "Reads SICK file. Take note of the header line.\n",
    "'''\n",
    "def read_input_file(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        sentences_A = []\n",
    "        sentencesA_length = []\n",
    "        sentences_B = []\n",
    "        sentencesB_length = []\n",
    "        relatedness_scores = []\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: break\n",
    "\n",
    "            sentence_A=line.split('\\t')[1]\n",
    "            sentence_B=line.split('\\t')[2]\n",
    "            relatedness_score=line.split('\\t')[3]    \n",
    "            \n",
    "            words = sentence_A.split()\n",
    "            sentencesA_length.append(len(words))\n",
    "            sentences_A.append(words)\n",
    "            \n",
    "            words = sentence_B.split()\n",
    "            sentencesB_length.append(len(words))\n",
    "            sentences_B.append(words)\n",
    "            \n",
    "            relatedness_scores.append(((float(relatedness_score) - 1) / 4 )) # convert scores to [0,1] values\n",
    "    assert len(sentences_A)==len(sentencesA_length)==len(sentences_B)==len(sentencesB_length)==len(relatedness_scores)\n",
    "    return Input(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "\n",
    "\n",
    "def generate_word2id_dictionary(texts, min_freq=-1, insert_words=None, lowercase=False, replace_digits=False):\n",
    "    counter = collections.Counter()\n",
    "    for text in texts:\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        if replace_digits:\n",
    "            text = re.sub(r'\\d', '0', text)\n",
    "        counter.update(text.strip().split())\n",
    "\n",
    "    word2id = collections.OrderedDict()\n",
    "    if insert_words is not None:\n",
    "        for word in insert_words:\n",
    "            word2id[word] = len(word2id)\n",
    "    word_count_list = counter.most_common()\n",
    "\n",
    "    for (word, count) in word_count_list:\n",
    "        if min_freq <= 0 or count >= min_freq:\n",
    "            word2id[word] = len(word2id)\n",
    "\n",
    "    return word2id\n",
    "\n",
    "# Create an OrderedDict of words and characters and their ids based on their frequency\n",
    "dataset = read_input_file(FLAGS.all_data)\n",
    "sentences = dataset.sentences_A + dataset.sentences_B\n",
    "word2id = generate_word2id_dictionary([\" \".join(sentence) for sentence in sentences], \n",
    "                                        int(FLAGS.min_word_freq), \n",
    "                                        insert_words=[\"<unk>\"], \n",
    "                                        lowercase=False, \n",
    "                                        replace_digits=False)\n",
    "char2id = generate_word2id_dictionary([\" \".join([\" \".join(list(word)) for word in sentence]) for sentence in sentences], \n",
    "                                        min_freq=-1, \n",
    "                                        insert_words=[\"<cunk>\"], \n",
    "                                        lowercase=False, \n",
    "                                        replace_digits=False)\n",
    "\n",
    "\n",
    "\n",
    "## vocab size\n",
    "word_vocab_size = len(word2id)\n",
    "\n",
    "char_vocab_size = len(char2id)\n",
    "print(word_vocab_size, char_vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "# Given the word2id, load its pretrained vectors into memory\n",
    "def preload_vectors(word2vec_path, word2id, vocab_size, emb_dim):\n",
    "    if word2vec_path:\n",
    "        print('Load word2vec_norm file {}'.format(word2vec_path))\n",
    "        with open(word2vec_path,'r') as f:\n",
    "            header=f.readline()\n",
    "            print(vocab_size, emb_dim)\n",
    "            scale = np.sqrt(3.0 / emb_dim)\n",
    "            init_W = np.random.uniform(-scale, scale, [vocab_size, emb_dim])\n",
    "            \n",
    "            print('vocab_size={}'.format(vocab_size))\n",
    "            while True:\n",
    "                line=f.readline()\n",
    "                if not line:break\n",
    "                word=line.split()[0]\n",
    "                if word in word2id:\n",
    "                    init_W[word2id[word]] = np.array(line.split()[1:], dtype = \"float32\")\n",
    "    return init_W\n",
    "\n",
    "init_W = preload_vectors(FLAGS.word2vec_path, word2id, word_vocab_size, FLAGS.word_embedding_dim)\n",
    "\n",
    "print(\"preloaded vectors\")\n",
    "\n",
    "\n",
    "def map_text_to_ids_returnwords(text, word2id, start_token=None, end_token=None, unk_token=None, lowercase=False, replace_digits=False):\n",
    "    ids = []\n",
    "    sentence = []\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if replace_digits:\n",
    "        text = re.sub(r'\\d', '0', text)\n",
    "\n",
    "    if start_token != None:\n",
    "        text = start_token + \" \" + text\n",
    "    if end_token != None:\n",
    "        text = text + \" \" + end_token\n",
    "    for word in text.strip().split():\n",
    "        if word in word2id:\n",
    "            ids.append(word2id[word])\n",
    "            sentence.append(word)\n",
    "        elif unk_token != None:\n",
    "            ids.append(word2id[unk_token])\n",
    "            sentence.append(unk_token)\n",
    "    return ids, sentence\n",
    "\n",
    "\n",
    "\n",
    "def map_text_to_ids(text, word2id, start_token=None, end_token=None, unk_token=None, lowercase=False, replace_digits=False):\n",
    "    ids = []\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if replace_digits:\n",
    "        text = re.sub(r'\\d', '0', text)\n",
    "\n",
    "    if start_token != None:\n",
    "        text = start_token + \" \" + text\n",
    "    if end_token != None:\n",
    "        text = text + \" \" + end_token\n",
    "    for word in text.strip().split():\n",
    "        if word in word2id:\n",
    "            ids.append(word2id[word])\n",
    "        elif unk_token != None:\n",
    "            ids.append(word2id[unk_token])\n",
    "    return ids\n",
    "\n",
    "\n",
    "def read_dataset(filename, lowercase_words, lowercase_chars, replace_digits, word2id, char2id):\n",
    "    dataset = []\n",
    "    data = read_input_file(filename)\n",
    "    sentences = [data.sentences_A, data.sentences_B]\n",
    "    max_sentence_len = 0\n",
    "    max_word_len = 0\n",
    "    for i in range(len(data.sentences_A)):\n",
    "\n",
    "        # map text to ids\n",
    "        senA_word_ids, sentenceA = map_text_to_ids_returnwords(\" \".join(sentences[0][i]), word2id, None, None, \"<unk>\", lowercase=False, replace_digits=False)\n",
    "        senA_char_ids = [map_text_to_ids(\" \".join(list(word)), char2id, None, None, \"<cunk>\", lowercase=False, replace_digits=False) for word in sentences[0][i]]\n",
    "\n",
    "        senB_word_ids, sentenceB = map_text_to_ids_returnwords(\" \".join(sentences[1][i]), word2id, None, None, \"<unk>\", lowercase=False, replace_digits=False)\n",
    "        senB_char_ids = [map_text_to_ids(\" \".join(list(word)), char2id, None, None, \"<cunk>\", lowercase=False, replace_digits=False) for word in sentences[1][i]]\n",
    "\n",
    "        assert(len(senA_word_ids) == len(senA_char_ids))\n",
    "        assert(len(senB_word_ids) == len(senB_char_ids))\n",
    "        \n",
    "        \n",
    "        senA_len = len(senA_word_ids)\n",
    "        senA_words_len = [len(word) for word in senA_char_ids]\n",
    "        senB_len = len(senB_word_ids)\n",
    "        senB_words_len = [len(word) for word in senB_char_ids]\n",
    "        \n",
    "        max_sentence_len = max(max_sentence_len, senA_len, senB_len)\n",
    "        max_word_len = max(max(senA_words_len), max(senB_words_len), max_word_len)\n",
    "        \n",
    "        sentenceA = \" \".join(sentenceA)\n",
    "        sentenceB = \" \".join(sentenceB)\n",
    "        dataset.append((senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, data.relatedness_scores[i], sentenceA, sentenceB ))\n",
    "    return dataset, max_sentence_len, max_word_len\n",
    "\n",
    "\n",
    "data_dev, max_sentence_len_dev, max_word_len_dev = read_dataset(FLAGS.validation_data, False, False, False, word2id, char2id)\n",
    "data_test, max_sentence_len_test, max_word_len_test = read_dataset(FLAGS.test_data, False, False, False, word2id, char2id)\n",
    "print(\"dataset read\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def next_batch(start,end,input):\n",
    "    senA_word_ids = input[0][start:end]\n",
    "    senA_len = input[1][start:end]\n",
    "    senA_char_ids = input[2][start:end]\n",
    "    senA_words_len = input[3][start:end]\n",
    "    senB_word_ids = input[4][start:end]\n",
    "    senB_len = input[5][start:end]\n",
    "    senB_char_ids = input[6][start:end]\n",
    "    senB_words_len = input[7][start:end]\n",
    "    relatedness_scores = np.reshape(input[8][start:end],(-1))\n",
    "    return Batch(senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, relatedness_scores)\n",
    "\n",
    "\n",
    "class Batch(object):\n",
    "    def __init__(self,senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, relatedness_scores):\n",
    "        self.senA_word_ids = senA_word_ids\n",
    "        self.senA_len = senA_len\n",
    "        self.senA_char_ids = senA_char_ids\n",
    "        self.senA_words_len = senA_words_len\n",
    "        self.senB_word_ids = senB_word_ids\n",
    "        self.senB_len = senB_len\n",
    "        self.senB_char_ids = senB_char_ids\n",
    "        self.senB_words_len = senB_words_len\n",
    "        self.relatedness_scores = relatedness_scores\n",
    "    def senA_word_ids(self):\n",
    "        return self.senA_word_ids\n",
    "    \n",
    "    def senA_len(self):\n",
    "        return self.senA_len\n",
    "    \n",
    "    def senA_char_ids(self):\n",
    "        return self.senA_char_ids\n",
    "    \n",
    "    def senA_words_len(self):\n",
    "        return self.senA_words_len\n",
    "    \n",
    "    def senB_word_ids(self):\n",
    "        return self.senB_word_ids\n",
    "    \n",
    "    def senB_len(self):\n",
    "        return self.senB_len\n",
    "    \n",
    "    def senB_char_ids(self):\n",
    "        return self.senB_char_ids\n",
    "    \n",
    "    def senB_words_len(self):\n",
    "        return self.senB_words_len\n",
    "    \n",
    "    def relatedness_scores(self):\n",
    "        return self.relatedness_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pad_to_max_len_sentence_and_word_len(data, max_sentence_len, max_word_len):\n",
    "    dataset = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        senA_word_ids = data[i][0]\n",
    "        senB_word_ids = data[i][4]\n",
    "        senA_char_ids = data[i][2]\n",
    "        senB_char_ids = data[i][6]\n",
    "        senA_words_len = data[i][3]\n",
    "        senB_words_len = data[i][7]\n",
    "        senA_len = data[i][1]\n",
    "        senB_len = data[i][5]\n",
    "        relatedness_scores = data[i][8]\n",
    "        sentencesA = data[i][9]\n",
    "        sentencesB = data[i][10]\n",
    "        \n",
    "        # pad values to max_sentence_len \n",
    "        senA_word_ids += [0] * (max_sentence_len - len(senA_word_ids))\n",
    "        senB_word_ids += [0] * (max_sentence_len - len(senB_word_ids))\n",
    "               \n",
    "        assert(len(senA_word_ids) == len(senB_word_ids))\n",
    "        \n",
    "        # pad values to max_word_len and pad each sentence to max_sentence_len\n",
    "        senA_char_ids_wl = [word + ([0] * (max_word_len - len(word))) for word in senA_char_ids]\n",
    "        senA_char_ids_sl = [([0] * (max_word_len))] * (max_sentence_len - len(senA_char_ids_wl))\n",
    "        senA_char_ids = senA_char_ids_wl + senA_char_ids_sl\n",
    "\n",
    "        senB_char_ids_wl = [word + ([0] * (max_word_len - len(word))) for word in senB_char_ids]\n",
    "        senB_char_ids_sl = [([0] * (max_word_len))] * (max_sentence_len - len(senB_char_ids_wl))\n",
    "        senB_char_ids = senB_char_ids_wl + senB_char_ids_sl\n",
    "\n",
    "        # pad word lengths to max_sentence_len\n",
    "        senA_words_len = senA_words_len + ([0] * (max_sentence_len - len(senA_words_len)))\n",
    "        senB_words_len = senB_words_len + ([0] * (max_sentence_len - len(senB_words_len)))\n",
    "        \n",
    "        dataset.append((senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, relatedness_scores, sentencesA, sentencesB))\n",
    "    return dataset\n",
    "\n",
    "print(\"pad_to_max_len_sentence_and_word_len\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def build_model(input_,input_length,dropout_, num_units):##should try this\n",
    "    rnn_cell=tf.nn.rnn_cell.LSTMCell(num_units)##why 50?\n",
    "    rnn_cell=tf.nn.rnn_cell.DropoutWrapper(rnn_cell,output_keep_prob=dropout_)\n",
    "    rnn_cell=tf.nn.rnn_cell.MultiRNNCell([rnn_cell]*config.num_layer)\n",
    "        \n",
    "    outputs,last_states=tf.nn.dynamic_rnn(\n",
    "        cell=rnn_cell,\n",
    "        dtype=data_type(),\n",
    "        sequence_length=input_length,\n",
    "        inputs=input_\n",
    "    )\n",
    "    return outputs,last_states\n",
    "\n",
    "print(\"build_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "SICK_new_train.txt_50\n",
      "32\n",
      "16\n",
      "training..\n",
      ">>: Model/siamese_final/RNN/MultiRNNCell/Cell0/LSTMCell/W_0:0\n",
      ">>: Model/siamese_final/RNN/MultiRNNCell/Cell0/LSTMCell/B:0\n",
      "Total batch size: 116, data size: 3500, batch size: 30\n",
      "1.0 0.5 0.98 8 10 1\n",
      "Epoch 0 Learning rate: 0.00999999977648\n",
      "Average cost:\t0.0686540445795 >\n",
      "Valid cost:\t0.0503565557301 >\n",
      "patience: 5\n",
      "Epoch 1 Learning rate: 0.00999999977648\n",
      "Average cost:\t0.0443619939595 >\n",
      "Valid cost:\t0.043319080025 >\n",
      "patience: 5\n",
      "Epoch 2 Learning rate: 0.00999999977648\n",
      "Average cost:\t0.0358971031704 >\n",
      "Valid cost:\t0.0390223786235 >\n",
      "patience: 5\n",
      "Epoch 3 Learning rate: 0.00999999977648\n",
      "Average cost:\t0.0302241912336 >\n",
      "Valid cost:\t0.0375324040651 >\n",
      "patience: 5\n",
      "Epoch 4 Learning rate: 0.00999999977648\n",
      "Average cost:\t0.0265551747931 >\n",
      "Valid cost:\t0.0360967665911 >\n",
      "patience: 5\n",
      "Epoch 5 Learning rate: 0.00999999977648\n",
      "Average cost:\t0.023290133754 >\n",
      "Valid cost:\t0.0349853113294 >\n",
      "patience: 5\n",
      "Epoch 6 Learning rate: 0.00999999977648\n",
      "Average cost:\t0.0202709270384 >\n",
      "Valid cost:\t0.0345025844872 >\n",
      "patience: 5\n",
      "Epoch 7 Learning rate: 0.00999999977648\n",
      "Average cost:\t0.0182311330191 >\n",
      "Valid cost:\t0.0337447784841 >\n",
      "patience: 5\n",
      "Epoch 8 Learning rate: 0.00980000011623\n",
      "Average cost:\t0.0170450650139 >\n",
      "Valid cost:\t0.033352624625 >\n",
      "patience: 5\n",
      "Epoch 9 Learning rate: 0.00960399955511\n",
      "Average cost:\t0.0157985819688 >\n",
      "Valid cost:\t0.0323164835572 >\n",
      "patience: 5\n",
      "0.0321844\n",
      "Pearson: (0.71755962325841927, 0.0)\n",
      "Execution time: 1487102359.46\n"
     ]
    }
   ],
   "source": [
    "for n in num_units:\n",
    "    print(n)\n",
    "    for file in training_files:\n",
    "\n",
    "        data_train, max_sentence_len_train, max_word_len_train = read_dataset(FLAGS.train_data+file, False, False, False, word2id, char2id)\n",
    "        file = file+\"_\"+str(n)\n",
    "        print(file)\n",
    "        \n",
    "        '''\n",
    "        Get the max_sentence_len and max_word_len\n",
    "        '''\n",
    "        ## Check the sentence\n",
    "        # max_sentence_len = sorted(([(len(sentence), sentence) for sentence in sentences]))[-1]\n",
    "        max_sentence_len = max(max_sentence_len_train, max_sentence_len_test, max_sentence_len_dev)\n",
    "        print(max_sentence_len)\n",
    "\n",
    "        ## Check the word\n",
    "        # max_word_len = sorted(([(len(word), word) for word in word2id]))[-1]\n",
    "        max_word_len = max(max_word_len_train, max_word_len_test, max_word_len_dev)\n",
    "        print(max_word_len)\n",
    "\n",
    "\n",
    "        data_train = pad_to_max_len_sentence_and_word_len(data_train, max_sentence_len, max_word_len)\n",
    "\n",
    "        data_dev = pad_to_max_len_sentence_and_word_len(data_dev, max_sentence_len, max_word_len)\n",
    "        data_test = pad_to_max_len_sentence_and_word_len(data_test, max_sentence_len, max_word_len)\n",
    "\n",
    "        zipped_train = zip(*data_train)\n",
    "        zipped_dev = zip(*data_dev)\n",
    "        zipped_test = zip(*data_test)\n",
    "        len(zipped_train[3][0])\n",
    "        print(\"training..\")\n",
    "\n",
    "        with tf.Graph().as_default():\n",
    "            initializer=tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            with tf.variable_scope('Model',initializer=initializer):\n",
    "\n",
    "                # word embedding\n",
    "                sentences_A = tf.placeholder(tf.int32, shape = ([None, max_sentence_len]), name='sentences_A')\n",
    "                sentencesA_length = tf.placeholder(tf.int32, shape=([None]),name='sentencesA_length')\n",
    "                sentences_B = tf.placeholder(tf.int32, shape = ([None, max_sentence_len]), name='sentences_B')\n",
    "                sentencesB_length = tf.placeholder(tf.int32, shape=([None]), name='sentencesB_length')\n",
    "                labels = tf.placeholder(tf.float32, shape=([None]),name='relatedness_score_label')\n",
    "                dropout_f = tf.placeholder(tf.float32)\n",
    "\n",
    "                # fine-tune by setting trainable to True\n",
    "                W = tf.Variable(tf.constant(0.0, shape = [word_vocab_size,FLAGS.word_embedding_dim]),trainable = False, name='word_embeddings')\n",
    "                embedding_placeholder = tf.placeholder(tf.float32, shape = [word_vocab_size, FLAGS.word_embedding_dim])\n",
    "                embedding_init = W.assign(embedding_placeholder)\n",
    "\n",
    "                # sentences_A_word_emb (30, 32, 300)\n",
    "                sentences_A_word_emb = tf.nn.embedding_lookup(params=embedding_init,ids=sentences_A)\n",
    "                sentences_B_word_emb = tf.nn.embedding_lookup(params=embedding_init,ids=sentences_B)\n",
    "\n",
    "\n",
    "                final_emb_A = sentences_A_word_emb\n",
    "                final_emb_B = sentences_B_word_emb\n",
    "                seq_len_A = sentencesA_length\n",
    "                seq_len_B = sentencesB_length\n",
    "\n",
    "\n",
    "                # Feed the concatenation of embeddings to biLSTM\n",
    "                with tf.variable_scope('siamese_final') as scope:\n",
    "                    outputs_A,last_states_A = build_model(final_emb_A,seq_len_A,dropout_f, n)\n",
    "                    scope.reuse_variables()\n",
    "                    outputs_B,last_states_B = build_model(final_emb_B,seq_len_B,dropout_f, n)   \n",
    "\n",
    "                senRep_a = last_states_A[config.num_layer-1][1]\n",
    "                senRep_b = last_states_B[config.num_layer-1][1]\n",
    "\n",
    "                prediction=tf.exp(tf.mul(-1.0,tf.reduce_mean(tf.abs(tf.sub(last_states_A[config.num_layer-1][1],last_states_B[config.num_layer-1][1])),1)))\n",
    "                cost = tf.reduce_mean(tf.square(tf.sub(prediction, labels)))\n",
    "\n",
    "                lr = tf.Variable(0.0,trainable=False)\n",
    "                tvars = tf.trainable_variables()\n",
    "                grads,_ = tf.clip_by_global_norm(tf.gradients(cost,tvars),config.max_grad_norm)\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "                train_op = optimizer.apply_gradients(zip(grads,tvars),global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "                new_lr = tf.placeholder(tf.float32,shape=[],name='new_learning_rate')\n",
    "                lr_update = tf.assign(lr,new_lr)\n",
    "\n",
    "                for v in tf.trainable_variables():\n",
    "                    print(\">>:\", v.name)\n",
    "                saver = tf.train.Saver()\n",
    "\n",
    "                ## Launch training graph\n",
    "                with tf.Session(config=config_gpu) as sess:\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                    total_batch = int(len(zipped_train[0]) / config.batch_size)##this doesn't include the extra samples? It does. see below after the \"if\" block.\n",
    "                    print('Total batch size: {}, data size: {}, batch size: {}'.format(total_batch,len(zipped_train[0]),config.batch_size))\n",
    "                    print(config.max_grad_norm,config.keep_prob,config.lr_decay,config.lr_max_epoch,config.train_max_epoch,config.num_layer)\n",
    "                    # train\n",
    "                    prev_train_cost=1\n",
    "                    prev_valid_cost=1\n",
    "\n",
    "                    patience = config.patience\n",
    "                    train_costs = []\n",
    "                    valid_costs = []\n",
    "\n",
    "\n",
    "                    for epoch in range(config.train_max_epoch):\n",
    "                        lr_decay = config.lr_decay**max(epoch+1-config.lr_max_epoch,0.0)\n",
    "                        sess.run([lr,lr_update],feed_dict = {new_lr:config.learning_rate*lr_decay})\n",
    "\n",
    "                        print('Epoch {} Learning rate: {}'.format(epoch,sess.run(lr)))\n",
    "\n",
    "                        avg_cost=0.\n",
    "\n",
    "                        for i in range(total_batch):\n",
    "                            start = i*config.batch_size\n",
    "                            end = (i+1)*config.batch_size\n",
    "                            next_batch_input = next_batch(start,end,zipped_train)\n",
    "\n",
    "                            _,train_cost= sess.run([train_op,cost], feed_dict={   \n",
    "                                sentences_A: next_batch_input.senA_word_ids,\n",
    "                                sentencesA_length: next_batch_input.senA_len,\n",
    "                                sentences_B: next_batch_input.senB_word_ids,\n",
    "                                sentencesB_length: next_batch_input.senB_len,\n",
    "\n",
    "                                labels: next_batch_input.relatedness_scores,\n",
    "                                dropout_f: config.keep_prob,\n",
    "                                embedding_placeholder: init_W\n",
    "                                })\n",
    "                            avg_cost += train_cost\n",
    "\n",
    "\n",
    "\n",
    "                        start = total_batch*config.batch_size\n",
    "                        end = len(zipped_train[0])\n",
    "                        #check if the last trailing batch and handle it\n",
    "                        if not start == end:\n",
    "                            next_batch_input = next_batch(start,end,zipped_train)\n",
    "                            _,train_cost= sess.run([train_op,cost], feed_dict={                                         \n",
    "                                sentences_A: next_batch_input.senA_word_ids,\n",
    "                                sentencesA_length: next_batch_input.senA_len,\n",
    "                                sentences_B: next_batch_input.senB_word_ids,\n",
    "                                sentencesB_length: next_batch_input.senB_len,\n",
    "\n",
    "                                labels: next_batch_input.relatedness_scores,\n",
    "                                dropout_f: config.keep_prob,\n",
    "                                embedding_placeholder: init_W\n",
    "                                })\n",
    "                            avg_cost += train_cost\n",
    "\n",
    "                        if prev_train_cost >  avg_cost / total_batch: \n",
    "                            print('Average cost:\\t{} >'.format(avg_cost / total_batch))\n",
    "                        else: \n",
    "                            print('Average cost:\\t{} <'.format(avg_cost / total_batch))\n",
    "\n",
    "                        prev_train_cost = avg_cost / total_batch\n",
    "\n",
    "                        train_costs.append(prev_train_cost)\n",
    "\n",
    "\n",
    "                        # validation\n",
    "                        next_batch_input = next_batch(0, len(zipped_dev[0]), zipped_dev)\n",
    "                        valid_cost,valid_predict=sess.run([cost,prediction],feed_dict={\n",
    "                                sentences_A: next_batch_input.senA_word_ids,\n",
    "                                sentencesA_length: next_batch_input.senA_len,\n",
    "                                sentences_B: next_batch_input.senB_word_ids,\n",
    "                                sentencesB_length: next_batch_input.senB_len,\n",
    "\n",
    "                                labels: next_batch_input.relatedness_scores,\n",
    "                                dropout_f: config.keep_prob,\n",
    "                                embedding_placeholder: init_W\n",
    "                        })\n",
    "                        if prev_valid_cost > valid_cost: \n",
    "                            print('Valid cost:\\t{} >'.format(valid_cost))\n",
    "                        else: \n",
    "                            print('Valid cost:\\t{} <'.format(valid_cost))\n",
    "                        prev_valid_cost=valid_cost\n",
    "\n",
    "                        valid_costs.append(valid_cost)\n",
    "\n",
    "                        # early stopping\n",
    "                        if patience == 0:\n",
    "                            print(\"Lost patience:\", patience)\n",
    "                            break\n",
    "                        if prev_valid_cost > valid_cost:\n",
    "                            patience -= 1\n",
    "                        else:\n",
    "                            patience = 5\n",
    "                        print(\"patience:\", patience)\n",
    "\n",
    "                    saver.save(sess, FLAGS.save_path+FLAGS.model_name+file,global_step=config.train_max_epoch)\n",
    "\n",
    "                    # test\n",
    "                    next_batch_input = next_batch(0, len(zipped_test[0]), zipped_test)\n",
    "                    test_cost,test_predict,senRep_a,senRep_b=sess.run([cost,prediction,senRep_a,senRep_b],feed_dict={\n",
    "                                sentences_A: next_batch_input.senA_word_ids,\n",
    "                                sentencesA_length: next_batch_input.senA_len,\n",
    "                                sentences_B: next_batch_input.senB_word_ids,\n",
    "                                sentencesB_length: next_batch_input.senB_len,\n",
    "\n",
    "                                labels: next_batch_input.relatedness_scores,\n",
    "                                dropout_f: config.keep_prob,\n",
    "                                embedding_placeholder: init_W\n",
    "                    })\n",
    "                    print(test_cost)\n",
    "\n",
    "        filename = FLAGS.result_path + FLAGS.model_name+'_'+file\n",
    "                    \n",
    "        ##TESTING\n",
    "        np.savetxt(filename+\"_senRep_a\", senRep_a)\n",
    "        np.savetxt(filename+\"_senRep_b\", senRep_b)\n",
    "\n",
    "        matrix_a = senRep_a\n",
    "        matrix_b = senRep_b\n",
    "\n",
    "\n",
    "\n",
    "        with open(filename+'.txt','w') as fw:\n",
    "            labels = []\n",
    "            preds = []\n",
    "            for _ in range(len(test_predict)):\n",
    "                fw.write(str(next_batch_input.relatedness_scores[_])+'\\t'+str(test_predict[_])+'\\n')\n",
    "                labels.append(next_batch_input.relatedness_scores[_])\n",
    "                preds.append(test_predict[_])\n",
    "            print(\"Pearson:\", pearsonr(labels,preds))\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"Execution time:\", stop - start)\n",
    "\n",
    "        y1 = train_costs\n",
    "        y2 = valid_costs\n",
    "        x_axis = range(1, len(y1)+1)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x_axis, y1, label='Train Cost')\n",
    "        ax.plot(x_axis, y2, label='Validation Cost')\n",
    "        ax.legend()\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.title('Training vs Validation Cost')\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.grid()\n",
    "        plt.savefig(filename+\".png\")\n",
    "        # plt.show()\n",
    "\n",
    "        test_data = read_input_file(FLAGS.test_data)\n",
    "        cnt = int(FLAGS.cnt)\n",
    "        with open(filename+'.txt', 'r') as f:\n",
    "            a = []\n",
    "            b = []\n",
    "            for line in f:\n",
    "                a.append(float(line.strip().split('\\t')[0]))\n",
    "                b.append(float(line.strip().split('\\t')[1]))\n",
    "            #most dissimilar/similar actual/predictions values\n",
    "            res = [abs(a[i] - b[i]) for i in range(len(a))]\n",
    "            sort = sorted(range(len(res)), key=lambda k: res[k], reverse = True)\n",
    "            firstn = sort[0:cnt]\n",
    "            lastn = sort[-(cnt):len(sort)]\n",
    "            lastn = lastn[::-1]\n",
    "\n",
    "            #prediction scores\n",
    "            predscores = sorted(range(len(b)), key=lambda k: b[k], reverse = True)\n",
    "            highpreds = predscores[0:cnt]\n",
    "            lowpreds = predscores[-(cnt):len(predscores)]\n",
    "            lowpreds = lowpreds[::-1]\n",
    "\n",
    "        np.savetxt(filename+\"_firstn\", firstn)\n",
    "        np.savetxt(filename+\"_lastn\", lastn)\n",
    "        np.savetxt(filename+\"_highpreds\", highpreds)\n",
    "        np.savetxt(filename+\"_lowpreds\", lowpreds)    \n",
    "        with open(filename+'_samples.txt', 'w') as f:\n",
    "        #     print(\"Most dissimilar (actual - predicted)\\n\")\n",
    "            f.write(\"Most dissimilar (actual - predicted)\\n\")\n",
    "            for i in range(len(test_data.sentences_A)):\n",
    "                txt = str(next_batch_input.relatedness_scores[firstn[i]])+\"\\t\"+str(test_predict[firstn[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[firstn[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[firstn[i]])) +\"\\t\"+ str(zipped_test[9][firstn[i]])+\"\\t\"+str(zipped_test[10][firstn[i]])\n",
    "    #             print(txt)\n",
    "                f.write(txt+\"\\n\")\n",
    "                if i == cnt-1:\n",
    "                    break\n",
    "\n",
    "\n",
    "        #     print(\"Most similar (actual - predicted)\\n\")\n",
    "            f.write(\"\\nMost similar (actual - predicted)\\n\")\n",
    "            length = len(test_data.sentences_A)\n",
    "            for i in range(length):\n",
    "            #     print(i)\n",
    "                txt = str(next_batch_input.relatedness_scores[lastn[i]])+\"\\t\"+str(test_predict[lastn[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[lastn[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[lastn[i]]))+\"\\t\"+ str(zipped_test[9][lastn[i]])+\"\\t\"+str(zipped_test[10][lastn[i]])\n",
    "        #         print(txt)\n",
    "                f.write(txt+\"\\n\")\n",
    "                if i == cnt-1:\n",
    "                    break\n",
    "        #     print(\"Most dissimilar sentence pairs as predicted\\n\")\n",
    "            f.write(\"\\nMost dissimilar sentence pairs as predicted\\n\")\n",
    "            for i in range(len(test_data.sentences_A)):\n",
    "            #     print(i)\n",
    "                txt = str(next_batch_input.relatedness_scores[lowpreds[i]])+\"\\t\"+str(test_predict[lowpreds[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[lowpreds[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[lowpreds[i]]))+\"\\t\"+ str(zipped_test[9][lowpreds[i]])+\"\\t\"+str(zipped_test[10][lowpreds[i]])\n",
    "        #         print(txt)\n",
    "                f.write(txt+\"\\n\")\n",
    "                if i == cnt-1:\n",
    "                    break\n",
    "\n",
    "\n",
    "        #     print(\"Most similar sentence pairs as predicted\\n\")\n",
    "            f.write(\"\\nMost similar sentence pairs as predicted\\n\")\n",
    "            length = len(test_data.sentences_A)\n",
    "            for i in range(length):\n",
    "            #     print(i)\n",
    "                txt = str(next_batch_input.relatedness_scores[highpreds[i]])+\"\\t\"+str(test_predict[highpreds[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[highpreds[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[highpreds[i]]))+\"\\t\"+ str(zipped_test[9][highpreds[i]])+\"\\t\"+str(zipped_test[10][highpreds[i]])\n",
    "        #         print(txt)\n",
    "                f.write(txt+\"\\n\")\n",
    "                if i == cnt-1:\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
