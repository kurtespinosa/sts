{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "all_data=data/SICK/SICK_all.txt\n",
      "char_embedding_dim=50\n",
      "data_path=data\n",
      "min_word_freq=2\n",
      "model_name=01\n",
      "result_path=result/\n",
      "save_path=model/\n",
      "test_data=data/SICK/SICK_test_annotated.txt\n",
      "train_data=data/SICK/SICK_new_train.txt\n",
      "use_fp64=False\n",
      "validation_data=data/SICK/SICK_new_trial.txt\n",
      "word2vec_path=embeddings/GoogleNews-vectors-negative300.txt\n",
      "word_embedding_dim=300\n",
      "2556 54\n",
      "Load word2vec_norm file embeddings/GoogleNews-vectors-negative300.txt\n",
      "2556 300\n",
      "vocab_size=2556\n",
      "preloaded vectors\n",
      "dataset read\n",
      "32\n",
      "16\n",
      "pad_to_max_len_sentence_and_word_len\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import collections\n",
    "import re\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "#comment: for server only\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg') \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "\n",
    "# Model Hyperparameters\n",
    "flags=tf.flags\n",
    "\n",
    "flags.DEFINE_string('word2vec_path','embeddings/GoogleNews-vectors-negative300.txt','Word2vec file with pre-trained embeddings')\n",
    "flags.DEFINE_string('data_path','data','data set path')\n",
    "flags.DEFINE_string('min_word_freq','2','Minimum word frequency')\n",
    "flags.DEFINE_string('save_path','model/','STS model output directory')\n",
    "flags.DEFINE_string('result_path','result/','data set path')\n",
    "flags.DEFINE_string('test_data',\"data/SICK/SICK_test_annotated.txt\",'Test data')\n",
    "flags.DEFINE_string('validation_data',\"data/SICK/SICK_new_trial.txt\",'Validation data')\n",
    "flags.DEFINE_string('train_data',\"data/SICK/SICK_new_train.txt\",'Train data')\n",
    "flags.DEFINE_string('all_data',\"data/SICK/SICK_all.txt\",'Train data')\n",
    "flags.DEFINE_string('cnt',\"10\",'Number of samples to show')\n",
    "flags.DEFINE_string('model_name','01','Filename of the model file')\n",
    "flags.DEFINE_integer('word_embedding_dim',300,'Dimensionality of word embedding')\n",
    "flags.DEFINE_integer('char_embedding_dim',50,'Dimensionality of char embedding')\n",
    "flags.DEFINE_bool('use_fp64',False,'Train using 64-bit floats instead of 32bit floats')\n",
    "\n",
    "\n",
    "\n",
    "FLAGS=flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print('Parameters:')\n",
    "for attr,value in sorted(FLAGS.__flags.items()):\n",
    "    print('{}={}'.format(attr,value))\n",
    "    \n",
    "    \n",
    "\n",
    "# model parameters\n",
    "class Config(object):\n",
    "    init_scale=0.2\n",
    "    learning_rate=.01\n",
    "    max_grad_norm=1.\n",
    "    keep_prob=0.5\n",
    "    lr_decay=0.98\n",
    "    batch_size=30\n",
    "    lr_max_epoch=8##this is for learning rate epoch\n",
    "    train_max_epoch=60\n",
    "    num_layer=1\n",
    "    num_units=50\n",
    "    patience=5\n",
    "    \n",
    "config=Config()\n",
    "config_gpu = tf.ConfigProto()\n",
    "config_gpu.gpu_options.allow_growth = True\n",
    "    \n",
    "def data_type():\n",
    "    return tf.float64 if FLAGS.use_fp64 else tf.float32\n",
    "\n",
    "\n",
    "\n",
    "class Input(object):\n",
    "    def __init__(self,sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores):\n",
    "        self.sentences_A=sentences_A\n",
    "        self.sentencesA_length=sentencesA_length\n",
    "        self.sentences_B=sentences_B\n",
    "        self.sentencesB_length=sentencesB_length\n",
    "        self.relatedness_scores=relatedness_scores\n",
    "    \n",
    "    def sentences_A(self):\n",
    "        return self.sentences_A\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesA_length\n",
    "    \n",
    "    def sentences_B(self):\n",
    "        return self.sentences_B\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesB_length\n",
    "    \n",
    "    def relatedness_scores(self):\n",
    "        return self.relatedness_scores\n",
    "\n",
    "\n",
    "'''\n",
    "Reads SICK file. Take note of the header line.\n",
    "'''\n",
    "def read_input_file(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        f.readline() # removes header line/column labels\n",
    "        sentences_A = []\n",
    "        sentencesA_length = []\n",
    "        sentences_B = []\n",
    "        sentencesB_length = []\n",
    "        relatedness_scores = []\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: break\n",
    "\n",
    "            sentence_A=line.split('\\t')[1]\n",
    "            sentence_B=line.split('\\t')[2]\n",
    "            relatedness_score=line.split('\\t')[3]    \n",
    "            \n",
    "            words = sentence_A.split()\n",
    "            sentencesA_length.append(len(words))\n",
    "            sentences_A.append(words)\n",
    "            \n",
    "            words = sentence_B.split()\n",
    "            sentencesB_length.append(len(words))\n",
    "            sentences_B.append(words)\n",
    "            \n",
    "            relatedness_scores.append(((float(relatedness_score) - 1) / 4 )) # convert scores to [0,1] values\n",
    "    assert len(sentences_A)==len(sentencesA_length)==len(sentences_B)==len(sentencesB_length)==len(relatedness_scores)\n",
    "    return Input(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "\n",
    "\n",
    "def generate_word2id_dictionary(texts, min_freq=-1, insert_words=None, lowercase=False, replace_digits=False):\n",
    "    counter = collections.Counter()\n",
    "    for text in texts:\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        if replace_digits:\n",
    "            text = re.sub(r'\\d', '0', text)\n",
    "        counter.update(text.strip().split())\n",
    "\n",
    "    word2id = collections.OrderedDict()\n",
    "    if insert_words is not None:\n",
    "        for word in insert_words:\n",
    "            word2id[word] = len(word2id)\n",
    "    word_count_list = counter.most_common()\n",
    "\n",
    "    for (word, count) in word_count_list:\n",
    "        if min_freq <= 0 or count >= min_freq:\n",
    "            word2id[word] = len(word2id)\n",
    "\n",
    "    return word2id\n",
    "\n",
    "# Create an OrderedDict of words and characters and their ids based on their frequency\n",
    "dataset = read_input_file(FLAGS.all_data)\n",
    "sentences = dataset.sentences_A + dataset.sentences_B\n",
    "word2id = generate_word2id_dictionary([\" \".join(sentence) for sentence in sentences], \n",
    "                                        int(FLAGS.min_word_freq), \n",
    "                                        insert_words=[\"<unk>\"], \n",
    "                                        lowercase=False, \n",
    "                                        replace_digits=False)\n",
    "char2id = generate_word2id_dictionary([\" \".join([\" \".join(list(word)) for word in sentence]) for sentence in sentences], \n",
    "                                        min_freq=-1, \n",
    "                                        insert_words=[\"<cunk>\"], \n",
    "                                        lowercase=False, \n",
    "                                        replace_digits=False)\n",
    "\n",
    "\n",
    "\n",
    "## vocab size\n",
    "word_vocab_size = len(word2id)\n",
    "\n",
    "char_vocab_size = len(char2id)\n",
    "print(word_vocab_size, char_vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Given the word2id, load its pretrained vectors into memory\n",
    "def preload_vectors(word2vec_path, word2id, vocab_size, emb_dim):\n",
    "    if word2vec_path:\n",
    "        print('Load word2vec_norm file {}'.format(word2vec_path))\n",
    "        with open(word2vec_path,'r') as f:\n",
    "            header=f.readline()\n",
    "            print(vocab_size, emb_dim)\n",
    "            scale = np.sqrt(3.0 / emb_dim)\n",
    "            init_W = np.random.uniform(-scale, scale, [vocab_size, emb_dim])\n",
    "            \n",
    "            print('vocab_size={}'.format(vocab_size))\n",
    "            while True:\n",
    "                line=f.readline()\n",
    "                if not line:break\n",
    "                word=line.split()[0]\n",
    "                if word in word2id:\n",
    "                    init_W[word2id[word]] = np.array(line.split()[1:], dtype = \"float32\")\n",
    "    return init_W\n",
    "\n",
    "init_W = preload_vectors(FLAGS.word2vec_path, word2id, word_vocab_size, FLAGS.word_embedding_dim)\n",
    "\n",
    "print(\"preloaded vectors\")\n",
    "\n",
    "\n",
    "def map_text_to_ids(text, word2id, start_token=None, end_token=None, unk_token=None, lowercase=False, replace_digits=False):\n",
    "    ids = []\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if replace_digits:\n",
    "        text = re.sub(r'\\d', '0', text)\n",
    "\n",
    "    if start_token != None:\n",
    "        text = start_token + \" \" + text\n",
    "    if end_token != None:\n",
    "        text = text + \" \" + end_token\n",
    "    for word in text.strip().split():\n",
    "        if word in word2id:\n",
    "            ids.append(word2id[word])\n",
    "        elif unk_token != None:\n",
    "            ids.append(word2id[unk_token])\n",
    "    return ids\n",
    "\n",
    "\n",
    "def read_dataset(filename, lowercase_words, lowercase_chars, replace_digits, word2id, char2id):\n",
    "    dataset = []\n",
    "    data = read_input_file(filename)\n",
    "    sentences = [data.sentences_A, data.sentences_B]\n",
    "    max_sentence_len = 0\n",
    "    max_word_len = 0\n",
    "    for i in range(len(data.sentences_A)):\n",
    "\n",
    "        # map text to ids\n",
    "        senA_word_ids = map_text_to_ids(\" \".join(sentences[0][i]), word2id, None, None, \"<unk>\", lowercase=False, replace_digits=False)\n",
    "        senA_char_ids = [map_text_to_ids(\" \".join(list(word)), char2id, None, None, \"<cunk>\", lowercase=False, replace_digits=False) for word in sentences[0][i]]\n",
    "\n",
    "        senB_word_ids = map_text_to_ids(\" \".join(sentences[1][i]), word2id, None, None, \"<unk>\", lowercase=False, replace_digits=False)\n",
    "        senB_char_ids = [map_text_to_ids(\" \".join(list(word)), char2id, None, None, \"<cunk>\", lowercase=False, replace_digits=False) for word in sentences[1][i]]\n",
    "\n",
    "        assert(len(senA_word_ids) == len(senA_char_ids))\n",
    "        assert(len(senB_word_ids) == len(senB_char_ids))\n",
    "        \n",
    "        \n",
    "        senA_len = len(senA_word_ids)\n",
    "        senA_words_len = [len(word) for word in senA_char_ids]\n",
    "        senB_len = len(senB_word_ids)\n",
    "        senB_words_len = [len(word) for word in senB_char_ids]\n",
    "        \n",
    "        max_sentence_len = max(max_sentence_len, senA_len, senB_len)\n",
    "        max_word_len = max(max(senA_words_len), max(senB_words_len), max_word_len)\n",
    "        \n",
    "        dataset.append((senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, data.relatedness_scores[i]))\n",
    "    return dataset, max_sentence_len, max_word_len\n",
    "\n",
    "data_train, max_sentence_len_train, max_word_len_train = read_dataset(FLAGS.train_data, False, False, False, word2id, char2id)\n",
    "data_dev, max_sentence_len_dev, max_word_len_dev = read_dataset(FLAGS.validation_data, False, False, False, word2id, char2id)\n",
    "data_test, max_sentence_len_test, max_word_len_test = read_dataset(FLAGS.test_data, False, False, False, word2id, char2id)\n",
    "print(\"dataset read\")\n",
    "\n",
    "'''\n",
    "Get the max_sentence_len and max_word_len\n",
    "'''\n",
    "## Check the sentence\n",
    "# max_sentence_len = sorted(([(len(sentence), sentence) for sentence in sentences]))[-1]\n",
    "max_sentence_len = max(max_sentence_len_train, max_sentence_len_test, max_sentence_len_dev)\n",
    "print(max_sentence_len)\n",
    "\n",
    "## Check the word\n",
    "# max_word_len = sorted(([(len(word), word) for word in word2id]))[-1]\n",
    "max_word_len = max(max_word_len_train, max_word_len_test, max_word_len_dev)\n",
    "print(max_word_len)\n",
    "\n",
    "\n",
    "def pad_to_max_len_sentence_and_word_len(data, max_sentence_len, max_word_len):\n",
    "    dataset = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        senA_word_ids = data[i][0]\n",
    "        senB_word_ids = data[i][4]\n",
    "        senA_char_ids = data[i][2]\n",
    "        senB_char_ids = data[i][6]\n",
    "        senA_words_len = data[i][3]\n",
    "        senB_words_len = data[i][7]\n",
    "        senA_len = data[i][1]\n",
    "        senB_len = data[i][5]\n",
    "        relatedness_scores = data[i][8]\n",
    "        \n",
    "        # pad values to max_sentence_len \n",
    "        senA_word_ids += [0] * (max_sentence_len - len(senA_word_ids))\n",
    "        senB_word_ids += [0] * (max_sentence_len - len(senB_word_ids))\n",
    "               \n",
    "        assert(len(senA_word_ids) == len(senB_word_ids))\n",
    "        \n",
    "        # pad values to max_word_len and pad each sentence to max_sentence_len\n",
    "        senA_char_ids_wl = [word + ([0] * (max_word_len - len(word))) for word in senA_char_ids]\n",
    "        senA_char_ids_sl = [([0] * (max_word_len))] * (max_sentence_len - len(senA_char_ids_wl))\n",
    "        senA_char_ids = senA_char_ids_wl + senA_char_ids_sl\n",
    "\n",
    "        senB_char_ids_wl = [word + ([0] * (max_word_len - len(word))) for word in senB_char_ids]\n",
    "        senB_char_ids_sl = [([0] * (max_word_len))] * (max_sentence_len - len(senB_char_ids_wl))\n",
    "        senB_char_ids = senB_char_ids_wl + senB_char_ids_sl\n",
    "\n",
    "        # pad word lengths to max_sentence_len\n",
    "        senA_words_len = senA_words_len + ([0] * (max_sentence_len - len(senA_words_len)))\n",
    "        senB_words_len = senB_words_len + ([0] * (max_sentence_len - len(senB_words_len)))\n",
    "        \n",
    "        dataset.append((senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, relatedness_scores))\n",
    "    return dataset\n",
    "\n",
    "data_train = pad_to_max_len_sentence_and_word_len(data_train, max_sentence_len, max_word_len)\n",
    "data_dev = pad_to_max_len_sentence_and_word_len(data_dev, max_sentence_len, max_word_len)\n",
    "data_test = pad_to_max_len_sentence_and_word_len(data_test, max_sentence_len, max_word_len)\n",
    "\n",
    "zipped_train = zip(*data_train)\n",
    "zipped_dev = zip(*data_dev)\n",
    "zipped_test = zip(*data_test)\n",
    "len(zipped_train[3][0])\n",
    "\n",
    "print(\"pad_to_max_len_sentence_and_word_len\")\n",
    "\n",
    "\n",
    "def next_batch(start,end,input):\n",
    "    senA_word_ids = input[0][start:end]\n",
    "    senA_len = input[1][start:end]\n",
    "    senA_char_ids = input[2][start:end]\n",
    "    senA_words_len = input[3][start:end]\n",
    "    senB_word_ids = input[4][start:end]\n",
    "    senB_len = input[5][start:end]\n",
    "    senB_char_ids = input[6][start:end]\n",
    "    senB_words_len = input[7][start:end]\n",
    "    relatedness_scores = np.reshape(input[8][start:end],(-1))\n",
    "    return Batch(senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, relatedness_scores)\n",
    "\n",
    "\n",
    "class Batch(object):\n",
    "    def __init__(self,senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, relatedness_scores):\n",
    "        self.senA_word_ids = senA_word_ids\n",
    "        self.senA_len = senA_len\n",
    "        self.senA_char_ids = senA_char_ids\n",
    "        self.senA_words_len = senA_words_len\n",
    "        self.senB_word_ids = senB_word_ids\n",
    "        self.senB_len = senB_len\n",
    "        self.senB_char_ids = senB_char_ids\n",
    "        self.senB_words_len = senB_words_len\n",
    "        self.relatedness_scores = relatedness_scores\n",
    "    def senA_word_ids(self):\n",
    "        return self.senA_word_ids\n",
    "    \n",
    "    def senA_len(self):\n",
    "        return self.senA_len\n",
    "    \n",
    "    def senA_char_ids(self):\n",
    "        return self.senA_char_ids\n",
    "    \n",
    "    def senA_words_len(self):\n",
    "        return self.senA_words_len\n",
    "    \n",
    "    def senB_word_ids(self):\n",
    "        return self.senB_word_ids\n",
    "    \n",
    "    def senB_len(self):\n",
    "        return self.senB_len\n",
    "    \n",
    "    def senB_char_ids(self):\n",
    "        return self.senB_char_ids\n",
    "    \n",
    "    def senB_words_len(self):\n",
    "        return self.senB_words_len\n",
    "    \n",
    "    def relatedness_scores(self):\n",
    "        return self.relatedness_scores\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_model\n",
      "training..\n",
      ">>: Model/char_embeddings:0\n",
      ">>: Model/siamese_char/RNN/MultiRNNCell/Cell0/LSTMCell/W_0:0\n",
      ">>: Model/siamese_char/RNN/MultiRNNCell/Cell0/LSTMCell/B:0\n",
      ">>: Model/siamese_char/siamese_char_ff_a/C_ff_weights_a:0\n",
      ">>: Model/siamese_char/siamese_char_ff_a/C_ff_bias_a:0\n",
      ">>: Model/siamese_char/siamese_char_ff_b/C_ff_weights_b:0\n",
      ">>: Model/siamese_char/siamese_char_ff_b/C_ff_bias_b:0\n",
      "Total batch size: 116, data size: 3499, batch size: 30\n",
      "1.0 0.5 0.98 8 2 1\n",
      "Epoch 0 Learning rate: 0.00999999977648\n",
      "Average cost:\t0.000449467899984 >\n",
      "Valid cost:\t0.0589639656246 >\n",
      "patience: 5\n",
      "Epoch 1 Learning rate: 0.00999999977648\n",
      "Average cost:\t0.000445687436852 >\n",
      "Valid cost:\t0.0588242411613 >\n",
      "patience: 5\n",
      "0.0577774\n",
      "Pearson: (0.32351206552757295, 2.1851180419981456e-120)\n",
      "Execution time: 1486139471.49\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGHCAYAAACqI7gCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERERySwlMiIiIpJZ/w9u+hQGNrf0twAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104ff0cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def build_model(input_,input_length,dropout_):##should try this\n",
    "    rnn_cell=tf.nn.rnn_cell.LSTMCell(config.num_units)##why 50?\n",
    "    rnn_cell=tf.nn.rnn_cell.DropoutWrapper(rnn_cell,output_keep_prob=dropout_)\n",
    "    rnn_cell=tf.nn.rnn_cell.MultiRNNCell([rnn_cell]*config.num_layer)\n",
    "        \n",
    "    outputs,last_states=tf.nn.dynamic_rnn(\n",
    "        cell=rnn_cell,\n",
    "        dtype=data_type(),\n",
    "        sequence_length=input_length,\n",
    "        inputs=input_\n",
    "    )\n",
    "    return outputs,last_states\n",
    "\n",
    "print(\"build_model\")\n",
    "print(\"training..\")\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    initializer=tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "    with tf.variable_scope('Model',initializer=initializer):\n",
    "        \n",
    "#         # word embedding\n",
    "#         sentences_A = tf.placeholder(tf.int32, shape = ([None, max_sentence_len]), name='sentences_A')\n",
    "#         sentencesA_length = tf.placeholder(tf.int32, shape=([None]),name='sentencesA_length')\n",
    "#         sentences_B = tf.placeholder(tf.int32, shape = ([None, max_sentence_len]), name='sentences_B')\n",
    "#         sentencesB_length = tf.placeholder(tf.int32, shape=([None]), name='sentencesB_length')\n",
    "        labels = tf.placeholder(tf.float32, shape=([None]),name='relatedness_score_label')\n",
    "        dropout_f = tf.placeholder(tf.float32)\n",
    "\n",
    "#         # fine-tune by setting trainable to True\n",
    "#         W = tf.Variable(tf.constant(0.0, shape = [word_vocab_size,FLAGS.word_embedding_dim]),trainable = True, name='word_embeddings')\n",
    "#         embedding_placeholder = tf.placeholder(tf.float32, shape = [word_vocab_size, FLAGS.word_embedding_dim])\n",
    "#         embedding_init = W.assign(embedding_placeholder)\n",
    "\n",
    "#         # sentences_A_word_emb (30, 32, 300)\n",
    "#         sentences_A_word_emb = tf.nn.embedding_lookup(params=embedding_init,ids=sentences_A)\n",
    "#         sentences_B_word_emb = tf.nn.embedding_lookup(params=embedding_init,ids=sentences_B)\n",
    "        \n",
    "\n",
    "        #character embedding\n",
    "        # C (131, 50)\n",
    "        C = tf.Variable(tf.random_uniform([char_vocab_size, FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"char_embeddings\")\n",
    "        # sentences_words_A (30, 32, 16)\n",
    "        sentences_words_A = tf.placeholder(tf.int32, shape = ([None, max_sentence_len, max_word_len]), name='sentences_words_A')\n",
    "        # sentencesA_words_length (30, 32)\n",
    "        sentencesA_words_length = tf.placeholder(tf.int32, shape=([None, max_sentence_len]),name='sentencesA_words_length')    \n",
    "        \n",
    "        sentences_words_B = tf.placeholder(tf.int32, shape = ([None, max_sentence_len, max_word_len]), name='sentences_words_B')\n",
    "        sentencesB_words_length = tf.placeholder(tf.int32, shape=([None, max_sentence_len]),name='sentencesB_words_length')    \n",
    "        \n",
    "        # sentences_A_char_emb (30, 32, 16, 50)\n",
    "        sentences_A_char_emb = tf.nn.embedding_lookup(params = C, ids = sentences_words_A)\n",
    "        sentences_B_char_emb = tf.nn.embedding_lookup(params = C, ids = sentences_words_B)\n",
    "            \n",
    "        #reshape char embedding\n",
    "        # sentences_A_char_emb_1 (960, 16, 50)\n",
    "        sentences_A_char_emb_1 = tf.reshape(sentences_A_char_emb, shape = ([-1, max_word_len, FLAGS.char_embedding_dim]))\n",
    "        # sentencesA_words_length_1 (960,)\n",
    "        sentencesA_words_length_1 = tf.reshape(sentencesA_words_length, shape = ([-1]))\n",
    "        sentences_B_char_emb_1 = tf.reshape(sentences_B_char_emb, shape = ([-1, max_word_len, FLAGS.char_embedding_dim]))\n",
    "        sentencesB_words_length_1 = tf.reshape(sentencesB_words_length, shape = ([-1]))\n",
    "\n",
    "        with tf.variable_scope('siamese_char') as scope:\n",
    "            # feed to biLSTM\n",
    "            c_outputs_A,c_last_states_A = build_model(sentences_A_char_emb_1,sentencesA_words_length_1,dropout_f)\n",
    "            scope.reuse_variables()\n",
    "            c_outputs_B,c_last_states_B = build_model(sentences_B_char_emb_1,sentencesB_words_length_1,dropout_f)\n",
    "\n",
    "#             c_output_state_fw_A, c_output_state_bw_A = c_last_states_A\n",
    "\n",
    "            # Each of the two tuples above has c and h, cell state and output respectively.\n",
    "#             c_last_states_A_ct = tf.concat(2, [c_output_state_fw_A[0], c_output_state_bw_A[0]])\n",
    "            # we use [0] because are interested with the c.\n",
    "#             c_last_states_A_ct_ff = tf.reshape(c_last_states_A_ct[0], shape = ([-1, config.num_units * 2]))\n",
    "            # feed to feedforward layer hidden states of sentences A\n",
    "            with tf.variable_scope('siamese_char_ff_a') as scope:\n",
    "                # 50 x 50\n",
    "                C_ff_weights_a = tf.Variable(tf.random_uniform([config.num_units, FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"C_ff_weights_a\")\n",
    "                # 50\n",
    "                C_ff_bias_a = tf.Variable(tf.random_uniform([FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"C_ff_bias_a\")\n",
    "                # 960 x 50 * 50 x 50 \n",
    "                c_output_a = tf.add(tf.matmul(c_last_states_A[0].c, C_ff_weights_a), C_ff_bias_a)\n",
    "                # 960 x 16 x 50\n",
    "                c_output_a = tf.tanh(c_output_a)\n",
    "                # ? x 32 x 16 x 50\n",
    "                c_output_a = tf.reshape(c_output_a, shape = ([-1, max_sentence_len, FLAGS.char_embedding_dim]))\n",
    "            \n",
    "#             c_output_state_fw_B, c_output_state_bw_B = c_last_states_B\n",
    "            # TODO: Why [0] and [1]??\n",
    "#             c_last_states_B_ct = tf.concat(2, [c_output_state_fw_B[0], c_output_state_bw_B[0]])\n",
    "#             c_last_states_B_ct_ff = tf.reshape(c_last_states_B_ct[0], shape = ([-1, config.num_units * 2]))\n",
    "\n",
    "#             # feed to feedforward layer hidden states of sentences B\n",
    "            with tf.variable_scope('siamese_char_ff_b') as scope:\n",
    "                C_ff_weights_b = tf.Variable(tf.random_uniform([config.num_units, FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"C_ff_weights_b\")\n",
    "                C_ff_bias_b = tf.Variable(tf.random_uniform([FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"C_ff_bias_b\")\n",
    "                c_output_b = tf.add(tf.matmul(c_last_states_B[0].c, C_ff_weights_b), C_ff_bias_b)\n",
    "                c_output_b = tf.tanh(c_output_b)\n",
    "                c_output_b = tf.reshape(c_output_b, shape = ([-1, max_sentence_len, FLAGS.char_embedding_dim]))\n",
    "        \n",
    "        # concatenate char + word embeddings here\n",
    "        # TODO: what does concat mean here? add embeddings element-wise or concat them but on which axis?\n",
    "#         final_emb_A = tf.add(sentences_A_word_emb, c_output_a)\n",
    "#         final_emb_B = tf.add(sentences_B_word_emb, c_output_b)\n",
    "        # final_emb_A = tf.concat(2, [sentences_A_word_emb, c_output_a])\n",
    "        # final_emb_B = tf.concat(2, [sentences_B_word_emb, c_output_b])\n",
    "\n",
    "#         final_emb_A = c_output_a\n",
    "#         final_emb_B = c_output_b\n",
    "#         seq_len_A = sentencesA_words_length\n",
    "#         seq_len_B = sentencesB_words_length\n",
    "\n",
    "        # Feed the concatenation of embeddings to biLSTM\n",
    "#         with tf.variable_scope('siamese_final') as scope:\n",
    "#             outputs_A,last_states_A = build_model(final_emb_A,seq_len_A,dropout_f)\n",
    "#             scope.reuse_variables()\n",
    "#             outputs_B,last_states_B = build_model(final_emb_B,seq_len_B,dropout_f)\n",
    "\n",
    "#             # last_states_A_bw (30, 50)\n",
    "#             last_states_A_fw, last_states_A_bw =  last_states_A\n",
    "#             # last_states_A_ct (2, 30, 100)\n",
    "#             last_states_A_ct = tf.concat(2, [last_states_A_fw[0], last_states_A_bw[0]])\n",
    "#             # last_states_A_ct_c (30, 100)\n",
    "#             last_states_A_ct_c = last_states_A_ct[0]\n",
    "\n",
    "#             last_states_B_fw, last_states_B_bw =  last_states_B\n",
    "#             last_states_B_ct = tf.concat(2, [last_states_B_fw[0], last_states_B_bw[0]])\n",
    "#             last_states_B_ct_c = last_states_B_ct[0]            \n",
    "\n",
    "#         # Compare last hidden states of both batch of sentences\n",
    "#         # TODO: make sense of [0] \n",
    "        prediction = tf.exp(tf.mul(-1.0,tf.reduce_mean(tf.reduce_mean(tf.abs(tf.sub(c_output_b,c_output_a)),1),1)))\n",
    "        cost = tf.reduce_mean(tf.square(tf.sub(prediction, labels)))\n",
    "\n",
    "        lr = tf.Variable(0.0,trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads,_ = tf.clip_by_global_norm(tf.gradients(cost,tvars),config.max_grad_norm)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        train_op = optimizer.apply_gradients(zip(grads,tvars),global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        new_lr = tf.placeholder(tf.float32,shape=[],name='new_learning_rate')\n",
    "        lr_update = tf.assign(lr,new_lr)\n",
    "        \n",
    "        for v in tf.trainable_variables():\n",
    "            print(\">>:\", v.name)\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        ## Launch training graph\n",
    "        with tf.Session(config=config_gpu) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            total_batch = int(len(zipped_train[0]) / config.batch_size)##this doesn't include the extra samples? It does. see below after the \"if\" block.\n",
    "            print('Total batch size: {}, data size: {}, batch size: {}'.format(total_batch,len(zipped_train[0]),config.batch_size))\n",
    "            print(config.max_grad_norm,config.keep_prob,config.lr_decay,config.lr_max_epoch,config.train_max_epoch,config.num_layer)\n",
    "            # train\n",
    "            prev_train_cost=1\n",
    "            prev_valid_cost=1\n",
    "            \n",
    "            patience = config.patience\n",
    "            train_costs = []\n",
    "            valid_costs = []\n",
    "            \n",
    "            \n",
    "            for epoch in range(config.train_max_epoch):\n",
    "                lr_decay = config.lr_decay**max(epoch+1-config.lr_max_epoch,0.0)\n",
    "                sess.run([lr,lr_update],feed_dict = {new_lr:config.learning_rate*lr_decay})\n",
    "                \n",
    "                print('Epoch {} Learning rate: {}'.format(epoch,sess.run(lr)))\n",
    "                \n",
    "                avg_cost=0.\n",
    "                \n",
    "                for i in range(total_batch):\n",
    "#                     print(\"i:\", i)\n",
    "                    start = i*config.batch_size\n",
    "                    end = (i+1)*config.batch_size\n",
    "                    next_batch_input = next_batch(start,end,zipped_train)\n",
    "\n",
    "                    _,train_cost= sess.run([train_op,cost], feed_dict={   \n",
    "#                     cost,prediction,c_output_b, c_output_a= sess.run([cost,prediction,c_output_b, c_output_a], feed_dict={   \n",
    "#                     c_output_a,c_output_a_t, \n",
    "#                     c_output_a,c_output_a_t,c_output_a_m, C_ff_bias_a, C_ff_weights_a,sentences_A_char_emb_1, sentencesA_words_length_1,c_last_states_A= sess.run([sentences_A_char_emb_1, sentencesA_words_length_1,c_last_states_A,c_output_a_m, C_ff_bias_a, C_ff_weights_a,c_output_a_t, c_output_a], feed_dict={   \n",
    "#                     last_states_A_ct_c, sentencesA_words_length, sentencesA_words_length_1, C, sentences_A_char_emb_1, train_cost,  prediction, last_states_A_ct, last_states_A_bw, final_emb_A, c_last_states_A_ct_ff, c_last_states_A_ct, c_output_state_fw_A, sentences_A_char_emb,sentences_words_A, sentences_A_word_emb= sess.run([last_states_A_ct_c, sentencesA_words_length, sentencesA_words_length_1, C, sentences_A_char_emb_1, cost, prediction,  last_states_A_ct, last_states_A_bw, final_emb_A, c_last_states_A_ct_ff, c_last_states_A_ct, c_output_state_fw_A, sentences_A_char_emb,sentences_words_A, sentences_A_word_emb], feed_dict={                                         \n",
    "#                             sentences_A: next_batch_input.senA_word_ids,\n",
    "#                             sentencesA_length: next_batch_input.senA_len,\n",
    "#                             sentences_B: next_batch_input.senB_word_ids,\n",
    "#                             sentencesB_length: next_batch_input.senB_len,\n",
    "                            \n",
    "                            sentences_words_A: next_batch_input.senA_char_ids,\n",
    "                            sentencesA_words_length: next_batch_input.senA_words_len,\n",
    "                            sentences_words_B: next_batch_input.senB_char_ids,\n",
    "                            sentencesB_words_length: next_batch_input.senB_words_len,\n",
    "                            labels: next_batch_input.relatedness_scores,\n",
    "                            dropout_f: config.keep_prob\n",
    "#                             embedding_placeholder: init_W\n",
    "                        })\n",
    "#                     avg_cost += train_cost\n",
    "            \n",
    "#                     print(\"sentences_A_char_emb_1\", sentences_A_char_emb_1.shape)\n",
    "#                     print(\"sentencesA_words_length_1\", sentencesA_words_length_1.shape)\n",
    "#                     print(\"c_last_states_A\", len(c_last_states_A))\n",
    "# #                     print(\"c_last_states_A\", c_last_states_A[0].c.shape)\n",
    "# #                     print(\"C_ff_bias_a\", C_ff_bias_a[0].c.shape)\n",
    "# #                     print(\"c_output_a_m\", c_output_a_m.shape)\n",
    "#                     print(\"c_output_a_t\", c_output_a_t.shape)\n",
    "#                     print(\"c_output_a\", c_output_a.shape)\n",
    "#                     print(\"c_output_b\", c_output_b.shape)\n",
    "#                     print(\"prediction\", prediction.shape)\n",
    "#                     print(\"cost\", cost)\n",
    "#                     break\n",
    "                    \n",
    "                start = total_batch*config.batch_size\n",
    "                end = len(zipped_train[0])\n",
    "                #check if the last trailing batch and handle it\n",
    "                if not start == end:\n",
    "                    next_batch_input = next_batch(start,end,zipped_train)\n",
    "                    _,train_cost= sess.run([train_op,cost], feed_dict={                                         \n",
    "#                             sentences_A: next_batch_input.senA_word_ids,\n",
    "#                             sentencesA_length: next_batch_input.senA_len,\n",
    "#                             sentences_B: next_batch_input.senB_word_ids,\n",
    "#                             sentencesB_length: next_batch_input.senB_len,\n",
    "                            \n",
    "                            sentences_words_A: next_batch_input.senA_char_ids,\n",
    "                            sentencesA_words_length: next_batch_input.senA_words_len,\n",
    "                            sentences_words_B: next_batch_input.senB_char_ids,\n",
    "                            sentencesB_words_length: next_batch_input.senB_words_len,\n",
    "                            labels: next_batch_input.relatedness_scores,\n",
    "                            dropout_f: config.keep_prob\n",
    "#                             embedding_placeholder: init_W\n",
    "                        })\n",
    "                    avg_cost += train_cost\n",
    "                \n",
    "                if prev_train_cost >  avg_cost / total_batch: \n",
    "                    print('Average cost:\\t{} >'.format(avg_cost / total_batch))\n",
    "                else: \n",
    "                    print('Average cost:\\t{} <'.format(avg_cost / total_batch))\n",
    "\n",
    "                prev_train_cost = avg_cost / total_batch\n",
    "                \n",
    "                train_costs.append(avg_cost)\n",
    "\n",
    "                \n",
    "                # validation\n",
    "                next_batch_input = next_batch(0, len(zipped_dev[0]), zipped_dev)\n",
    "                valid_cost,valid_predict=sess.run([cost,prediction],feed_dict={\n",
    "#                         sentences_A: next_batch_input.senA_word_ids,\n",
    "#                         sentencesA_length: next_batch_input.senA_len,\n",
    "#                         sentences_B: next_batch_input.senB_word_ids,\n",
    "#                         sentencesB_length: next_batch_input.senB_len,\n",
    "\n",
    "                        sentences_words_A: next_batch_input.senA_char_ids,\n",
    "                        sentencesA_words_length: next_batch_input.senA_words_len,\n",
    "                        sentences_words_B: next_batch_input.senB_char_ids,\n",
    "                        sentencesB_words_length: next_batch_input.senB_words_len,\n",
    "                        labels: next_batch_input.relatedness_scores,\n",
    "                        dropout_f: config.keep_prob#,\n",
    "#                         embedding_placeholder: init_W\n",
    "                })\n",
    "                if prev_valid_cost > valid_cost: \n",
    "                    print('Valid cost:\\t{} >'.format(valid_cost))\n",
    "                else: \n",
    "                    print('Valid cost:\\t{} <'.format(valid_cost))\n",
    "                prev_valid_cost=valid_cost\n",
    "                \n",
    "                valid_costs.append(valid_cost)\n",
    "                \n",
    "                # early stopping\n",
    "                if patience == 0:\n",
    "                    print(\"Lost patience:\", patience)\n",
    "                    break\n",
    "                if prev_valid_cost > valid_cost:\n",
    "                    patience -= 1\n",
    "                else:\n",
    "                    patience = 5\n",
    "                print(\"patience:\", patience)\n",
    "#                 break\n",
    "            saver.save(sess, FLAGS.save_path+FLAGS.model_name,global_step=config.train_max_epoch)\n",
    "\n",
    "            # test\n",
    "            next_batch_input = next_batch(0, len(zipped_test[0]), zipped_test)\n",
    "            test_cost,test_predict=sess.run([cost,prediction],feed_dict={\n",
    "#                     sentences_A: next_batch_input.senA_word_ids,\n",
    "#                     sentencesA_length: next_batch_input.senA_len,\n",
    "#                     sentences_B: next_batch_input.senB_word_ids,\n",
    "#                     sentencesB_length: next_batch_input.senB_len,\n",
    "\n",
    "                    sentences_words_A: next_batch_input.senA_char_ids,\n",
    "                    sentencesA_words_length: next_batch_input.senA_words_len,\n",
    "                    sentences_words_B: next_batch_input.senB_char_ids,\n",
    "                    sentencesB_words_length: next_batch_input.senB_words_len,\n",
    "                    labels: next_batch_input.relatedness_scores,\n",
    "                    dropout_f: config.keep_prob#,\n",
    "#                     embedding_placeholder: init_W         \n",
    "            })\n",
    "            print(test_cost)\n",
    "\n",
    "\n",
    "            \n",
    "with open(FLAGS.result_path + FLAGS.model_name+'.txt','w') as fw:\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for _ in range(len(test_predict)):\n",
    "        fw.write(str(next_batch_input.relatedness_scores[_])+'\\t'+str(test_predict[_])+'\\n')\n",
    "        labels.append(next_batch_input.relatedness_scores[_])\n",
    "        preds.append(test_predict[_])\n",
    "    print(\"Pearson:\", pearsonr(labels,preds))\n",
    "stop = timeit.default_timer()\n",
    "print(\"Execution time:\", stop - start)\n",
    "\n",
    "\n",
    "y1 = train_costs\n",
    "y2 = valid_costs\n",
    "x_axis = range(1, len(y1)+1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis, y1, label='Train Cost')\n",
    "ax.plot(x_axis, y2, label='Validation Cost')\n",
    "ax.legend()\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training vs Validation Cost')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.savefig(FLAGS.result_path + FLAGS.model_name+\"_\"+train_max_epoch+\".png\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = read_input_file(FLAGS.test_data)\n",
    "cnt = int(FLAGS.cnt)\n",
    "with open(FLAGS.result_path + FLAGS.model_name+'.txt', 'r') as f:\n",
    "    a = []\n",
    "    b = []\n",
    "    for line in f:\n",
    "        a.append(float(line.strip().split('\\t')[0]))\n",
    "        b.append(float(line.strip().split('\\t')[1]))\n",
    "    #most dissimilar/similar actual/predictions values\n",
    "    res = [abs(a[i] - b[i]) for i in range(len(a))]\n",
    "    sort = sorted(range(len(res)), key=lambda k: res[k], reverse = True)\n",
    "    firstn = sort[0:cnt]\n",
    "    lastn = sort[-(cnt):len(sort)]\n",
    "    lastn = lastn[::-1]\n",
    "    \n",
    "    #prediction scores\n",
    "    predscores = sorted(range(len(b)), key=lambda k: b[k], reverse = True)\n",
    "    highpreds = predscores[0:cnt]\n",
    "    lowpreds = predscores[-(cnt):len(predscores)]\n",
    "    lowpreds = lowpreds[::-1]\n",
    "with open(FLAGS.result_path + FLAGS.model_name+'_samples.txt', 'w') as f:\n",
    "#     print(\"Most dissimilar (actual - predicted)\\n\")\n",
    "    f.write(\"Most dissimilar (actual - predicted)\\n\")\n",
    "    for i in range(len(test_data.sentences_A)):\n",
    "    #     print(i)\n",
    "        txt = str(next_batch_input.relatedness_scores[firstn[i]])+\"\\t\"+str(test_predict[firstn[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[firstn[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[firstn[i]]))\n",
    "#         print(txt)\n",
    "        f.write(txt+\"\\n\")\n",
    "        if i == cnt-1:\n",
    "            break\n",
    "\n",
    "\n",
    "#     print(\"Most similar (actual - predicted)\\n\")\n",
    "    f.write(\"Most similar (actual - predicted)\\n\")\n",
    "    length = len(test_data.sentences_A)\n",
    "    for i in range(length):\n",
    "    #     print(i)\n",
    "        txt = str(next_batch_input.relatedness_scores[lastn[i]])+\"\\t\"+str(test_predict[lastn[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[lastn[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[lastn[i]]))\n",
    "#         print(txt)\n",
    "        f.write(txt+\"\\n\")\n",
    "        if i == cnt-1:\n",
    "            break\n",
    "#     print(\"Most dissimilar sentence pairs as predicted\\n\")\n",
    "    f.write(\"Most dissimilar sentence pairs as predicted\\n\")\n",
    "    for i in range(len(test_data.sentences_A)):\n",
    "    #     print(i)\n",
    "        txt = str(next_batch_input.relatedness_scores[lowpreds[i]])+\"\\t\"+str(test_predict[lowpreds[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[lowpreds[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[lowpreds[i]]))\n",
    "#         print(txt)\n",
    "        f.write(txt+\"\\n\")\n",
    "        if i == cnt-1:\n",
    "            break\n",
    "\n",
    "\n",
    "#     print(\"Most similar sentence pairs as predicted\\n\")\n",
    "    f.write(\"Most similar sentence pairs as predicted\\n\")\n",
    "    length = len(test_data.sentences_A)\n",
    "    for i in range(length):\n",
    "    #     print(i)\n",
    "        txt = str(next_batch_input.relatedness_scores[highpreds[i]])+\"\\t\"+str(test_predict[highpreds[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[highpreds[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[highpreds[i]]))\n",
    "#         print(txt)\n",
    "        f.write(txt+\"\\n\")\n",
    "        if i == cnt-1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
