{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO for server\n",
    "'''\n",
    "1. uncomment the matplotlib command to generate images in the server\n",
    "2. comment plt.show()\n",
    "3. change train_max_epoch\n",
    "4. update model/expt #\n",
    "5. files to use\n",
    "6. change path of training to just be dir\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import collections\n",
    "import re\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "#comment: for server only\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "\n",
    "# Model Hyperparameters\n",
    "flags=tf.flags\n",
    "\n",
    "flags.DEFINE_string('word2vec_path','embeddings/GoogleNews-vectors-negative300.txt','Word2vec file with pre-trained embeddings')\n",
    "flags.DEFINE_string('data_path','data','data set path')\n",
    "flags.DEFINE_string('min_word_freq','2','Minimum word frequency')\n",
    "flags.DEFINE_string('save_path','model/','STS model output directory')\n",
    "flags.DEFINE_string('result_path','result/','data set path')\n",
    "flags.DEFINE_string('test_data',\"data/STS/test.tsv\",'Test data')\n",
    "flags.DEFINE_string('validation_data',\"data/STS/validation.tsv\",'Validation data')\n",
    "flags.DEFINE_string('train_data',\"data/STS/\",'Train data')\n",
    "flags.DEFINE_string('all_data',\"data/STS/alldata.tsv\",'Train data')\n",
    "flags.DEFINE_string('cnt',\"100\",'Number of samples to show')\n",
    "flags.DEFINE_string('model_name','14','Filename of the model file')\n",
    "flags.DEFINE_integer('word_embedding_dim',300,'Dimensionality of word embedding')\n",
    "flags.DEFINE_integer('char_embedding_dim',50,'Dimensionality of char embedding')\n",
    "flags.DEFINE_bool('use_fp64',False,'Train using 64-bit floats instead of 32bit floats')\n",
    "\n",
    "\n",
    "\n",
    "FLAGS=flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print('Parameters:')\n",
    "for attr,value in sorted(FLAGS.__flags.items()):\n",
    "    print('{}={}'.format(attr,value))\n",
    "    \n",
    "    \n",
    "\n",
    "# model parameters\n",
    "class Config(object):\n",
    "    init_scale=0.2\n",
    "    learning_rate=.01\n",
    "    max_grad_norm=1.\n",
    "    keep_prob=0.5\n",
    "    lr_decay=0.98\n",
    "    batch_size=30\n",
    "    lr_max_epoch=8##this is for learning rate epoch\n",
    "    train_max_epoch=100\n",
    "    num_layer=1\n",
    "    num_units=50\n",
    "    patience=5\n",
    "    \n",
    "config=Config()\n",
    "config_gpu = tf.ConfigProto()\n",
    "config_gpu.gpu_options.allow_growth = True\n",
    "    \n",
    "def data_type():\n",
    "    return tf.float64 if FLAGS.use_fp64 else tf.float32\n",
    "\n",
    "training_files = ['train.tsv']\n",
    "\n",
    "\n",
    "class Input(object):\n",
    "    def __init__(self,sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores):\n",
    "        self.sentences_A=sentences_A\n",
    "        self.sentencesA_length=sentencesA_length\n",
    "        self.sentences_B=sentences_B\n",
    "        self.sentencesB_length=sentencesB_length\n",
    "        self.relatedness_scores=relatedness_scores\n",
    "    \n",
    "    def sentences_A(self):\n",
    "        return self.sentences_A\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesA_length\n",
    "    \n",
    "    def sentences_B(self):\n",
    "        return self.sentences_B\n",
    "    \n",
    "    def sentencesA_length(self):\n",
    "        return self.sentencesB_length\n",
    "    \n",
    "    def relatedness_scores(self):\n",
    "        return self.relatedness_scores\n",
    "\n",
    "'''\n",
    "Reads STS file. \n",
    "'''\n",
    "def read_input_file(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        sentences_A = []\n",
    "        sentencesA_length = []\n",
    "        sentences_B = []\n",
    "        sentencesB_length = []\n",
    "        relatedness_scores = []\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line: break\n",
    "\n",
    "            sentence_A=line.split('\\t')[1]\n",
    "            sentence_B=line.split('\\t')[2]\n",
    "            relatedness_score=line.split('\\t')[0]    \n",
    "            \n",
    "            words = sentence_A.split()\n",
    "            sentencesA_length.append(len(words))\n",
    "            sentences_A.append(words)\n",
    "            \n",
    "            words = sentence_B.split()\n",
    "            sentencesB_length.append(len(words))\n",
    "            sentences_B.append(words)\n",
    "            \n",
    "            relatedness_scores.append((float(relatedness_score)/5))\n",
    "    assert len(sentences_A)==len(sentencesA_length)==len(sentences_B)==len(sentencesB_length)==len(relatedness_scores)\n",
    "    return Input(sentences_A,sentencesA_length,sentences_B,sentencesB_length,relatedness_scores)\n",
    "\n",
    "\n",
    "\n",
    "def generate_word2id_dictionary(texts, min_freq=-1, insert_words=None, lowercase=False, replace_digits=False):\n",
    "    counter = collections.Counter()\n",
    "    for text in texts:\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        if replace_digits:\n",
    "            text = re.sub(r'\\d', '0', text)\n",
    "        counter.update(text.strip().split())\n",
    "\n",
    "    word2id = collections.OrderedDict()\n",
    "    if insert_words is not None:\n",
    "        for word in insert_words:\n",
    "            word2id[word] = len(word2id)\n",
    "    word_count_list = counter.most_common()\n",
    "\n",
    "    for (word, count) in word_count_list:\n",
    "        if min_freq <= 0 or count >= min_freq:\n",
    "            word2id[word] = len(word2id)\n",
    "\n",
    "    return word2id\n",
    "\n",
    "# Create an OrderedDict of words and characters and their ids based on their frequency\n",
    "dataset = read_input_file(FLAGS.all_data)\n",
    "sentences = dataset.sentences_A + dataset.sentences_B\n",
    "word2id = generate_word2id_dictionary([\" \".join(sentence) for sentence in sentences], \n",
    "                                        int(FLAGS.min_word_freq), \n",
    "                                        insert_words=[\"<unk>\"], \n",
    "                                        lowercase=False, \n",
    "                                        replace_digits=False)\n",
    "char2id = generate_word2id_dictionary([\" \".join([\" \".join(list(word)) for word in sentence]) for sentence in sentences], \n",
    "                                        min_freq=-1, \n",
    "                                        insert_words=[\"<cunk>\"], \n",
    "                                        lowercase=False, \n",
    "                                        replace_digits=False)\n",
    "\n",
    "\n",
    "\n",
    "## vocab size\n",
    "word_vocab_size = len(word2id)\n",
    "\n",
    "char_vocab_size = len(char2id)\n",
    "print(word_vocab_size, char_vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "# Given the word2id, load its pretrained vectors into memory\n",
    "def preload_vectors(word2vec_path, word2id, vocab_size, emb_dim):\n",
    "    if word2vec_path:\n",
    "        print('Load word2vec_norm file {}'.format(word2vec_path))\n",
    "        with open(word2vec_path,'r') as f:\n",
    "            header=f.readline()\n",
    "            print(vocab_size, emb_dim)\n",
    "            scale = np.sqrt(3.0 / emb_dim)\n",
    "            init_W = np.random.uniform(-scale, scale, [vocab_size, emb_dim])\n",
    "            \n",
    "            print('vocab_size={}'.format(vocab_size))\n",
    "            while True:\n",
    "                line=f.readline()\n",
    "                if not line:break\n",
    "                word=line.split()[0]\n",
    "                if word in word2id:\n",
    "                    init_W[word2id[word]] = np.array(line.split()[1:], dtype = \"float32\")\n",
    "    return init_W\n",
    "\n",
    "init_W = preload_vectors(FLAGS.word2vec_path, word2id, word_vocab_size, FLAGS.word_embedding_dim)\n",
    "\n",
    "print(\"preloaded vectors\")\n",
    "\n",
    "\n",
    "def map_text_to_ids_returnwords(text, word2id, start_token=None, end_token=None, unk_token=None, lowercase=False, replace_digits=False):\n",
    "    ids = []\n",
    "    sentence = []\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if replace_digits:\n",
    "        text = re.sub(r'\\d', '0', text)\n",
    "\n",
    "    if start_token != None:\n",
    "        text = start_token + \" \" + text\n",
    "    if end_token != None:\n",
    "        text = text + \" \" + end_token\n",
    "    for word in text.strip().split():\n",
    "        if word in word2id:\n",
    "            ids.append(word2id[word])\n",
    "            sentence.append(word)\n",
    "        elif unk_token != None:\n",
    "            ids.append(word2id[unk_token])\n",
    "            sentence.append(unk_token)\n",
    "    return ids, sentence\n",
    "\n",
    "\n",
    "\n",
    "def map_text_to_ids(text, word2id, start_token=None, end_token=None, unk_token=None, lowercase=False, replace_digits=False):\n",
    "    ids = []\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if replace_digits:\n",
    "        text = re.sub(r'\\d', '0', text)\n",
    "\n",
    "    if start_token != None:\n",
    "        text = start_token + \" \" + text\n",
    "    if end_token != None:\n",
    "        text = text + \" \" + end_token\n",
    "    for word in text.strip().split():\n",
    "        if word in word2id:\n",
    "            ids.append(word2id[word])\n",
    "        elif unk_token != None:\n",
    "            ids.append(word2id[unk_token])\n",
    "    return ids\n",
    "\n",
    "\n",
    "def read_dataset(filename, lowercase_words, lowercase_chars, replace_digits, word2id, char2id):\n",
    "    dataset = []\n",
    "    data = read_input_file(filename)\n",
    "    sentences = [data.sentences_A, data.sentences_B]\n",
    "    max_sentence_len = 0\n",
    "    max_word_len = 0\n",
    "    for i in range(len(data.sentences_A)):\n",
    "\n",
    "        # map text to ids\n",
    "        senA_word_ids, sentenceA = map_text_to_ids_returnwords(\" \".join(sentences[0][i]), word2id, None, None, \"<unk>\", lowercase=False, replace_digits=False)\n",
    "        senA_char_ids = [map_text_to_ids(\" \".join(list(word)), char2id, None, None, \"<cunk>\", lowercase=False, replace_digits=False) for word in sentences[0][i]]\n",
    "\n",
    "        senB_word_ids, sentenceB = map_text_to_ids_returnwords(\" \".join(sentences[1][i]), word2id, None, None, \"<unk>\", lowercase=False, replace_digits=False)\n",
    "        senB_char_ids = [map_text_to_ids(\" \".join(list(word)), char2id, None, None, \"<cunk>\", lowercase=False, replace_digits=False) for word in sentences[1][i]]\n",
    "\n",
    "        assert(len(senA_word_ids) == len(senA_char_ids))\n",
    "        assert(len(senB_word_ids) == len(senB_char_ids))\n",
    "        \n",
    "        \n",
    "        senA_len = len(senA_word_ids)\n",
    "        senA_words_len = [len(word) for word in senA_char_ids]\n",
    "        senB_len = len(senB_word_ids)\n",
    "        senB_words_len = [len(word) for word in senB_char_ids]\n",
    "        \n",
    "        max_sentence_len = max(max_sentence_len, senA_len, senB_len)\n",
    "        max_word_len = max(max(senA_words_len), max(senB_words_len), max_word_len)\n",
    "        \n",
    "        sentenceA = \" \".join(sentenceA)\n",
    "        sentenceB = \" \".join(sentenceB)\n",
    "        dataset.append((senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, data.relatedness_scores[i], sentenceA, sentenceB ))\n",
    "    return dataset, max_sentence_len, max_word_len\n",
    "\n",
    "\n",
    "data_dev, max_sentence_len_dev, max_word_len_dev = read_dataset(FLAGS.validation_data, False, False, False, word2id, char2id)\n",
    "data_test, max_sentence_len_test, max_word_len_test = read_dataset(FLAGS.test_data, False, False, False, word2id, char2id)\n",
    "print(\"dataset read\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def next_batch(start,end,input):\n",
    "    senA_word_ids = input[0][start:end]\n",
    "    senA_len = input[1][start:end]\n",
    "    senA_char_ids = input[2][start:end]\n",
    "    senA_words_len = input[3][start:end]\n",
    "    senB_word_ids = input[4][start:end]\n",
    "    senB_len = input[5][start:end]\n",
    "    senB_char_ids = input[6][start:end]\n",
    "    senB_words_len = input[7][start:end]\n",
    "    relatedness_scores = np.reshape(input[8][start:end],(-1))\n",
    "    return Batch(senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, relatedness_scores)\n",
    "\n",
    "\n",
    "class Batch(object):\n",
    "    def __init__(self,senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, relatedness_scores):\n",
    "        self.senA_word_ids = senA_word_ids\n",
    "        self.senA_len = senA_len\n",
    "        self.senA_char_ids = senA_char_ids\n",
    "        self.senA_words_len = senA_words_len\n",
    "        self.senB_word_ids = senB_word_ids\n",
    "        self.senB_len = senB_len\n",
    "        self.senB_char_ids = senB_char_ids\n",
    "        self.senB_words_len = senB_words_len\n",
    "        self.relatedness_scores = relatedness_scores\n",
    "    def senA_word_ids(self):\n",
    "        return self.senA_word_ids\n",
    "    \n",
    "    def senA_len(self):\n",
    "        return self.senA_len\n",
    "    \n",
    "    def senA_char_ids(self):\n",
    "        return self.senA_char_ids\n",
    "    \n",
    "    def senA_words_len(self):\n",
    "        return self.senA_words_len\n",
    "    \n",
    "    def senB_word_ids(self):\n",
    "        return self.senB_word_ids\n",
    "    \n",
    "    def senB_len(self):\n",
    "        return self.senB_len\n",
    "    \n",
    "    def senB_char_ids(self):\n",
    "        return self.senB_char_ids\n",
    "    \n",
    "    def senB_words_len(self):\n",
    "        return self.senB_words_len\n",
    "    \n",
    "    def relatedness_scores(self):\n",
    "        return self.relatedness_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_model(input_,input_length,dropout_):##should try this\n",
    "    rnn_cell=tf.nn.rnn_cell.LSTMCell(config.num_units)##why 50?\n",
    "    rnn_cell=tf.nn.rnn_cell.DropoutWrapper(rnn_cell,output_keep_prob=dropout_)\n",
    "    rnn_cell=tf.nn.rnn_cell.MultiRNNCell([rnn_cell]*config.num_layer)\n",
    "        \n",
    "    outputs,last_states=tf.nn.dynamic_rnn(\n",
    "        cell=rnn_cell,\n",
    "        dtype=data_type(),\n",
    "        sequence_length=input_length,\n",
    "        inputs=input_\n",
    "    )\n",
    "    return outputs,last_states\n",
    "\n",
    "print(\"build_model\")\n",
    "\n",
    "\n",
    "def pad_to_max_len_sentence_and_word_len(data, max_sentence_len, max_word_len):\n",
    "    dataset = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        senA_word_ids = data[i][0]\n",
    "        senB_word_ids = data[i][4]\n",
    "        senA_char_ids = data[i][2]\n",
    "        senB_char_ids = data[i][6]\n",
    "        senA_words_len = data[i][3]\n",
    "        senB_words_len = data[i][7]\n",
    "        senA_len = data[i][1]\n",
    "        senB_len = data[i][5]\n",
    "        relatedness_scores = data[i][8]\n",
    "        sentencesA = data[i][9]\n",
    "        sentencesB = data[i][10]\n",
    "        \n",
    "        # pad values to max_sentence_len \n",
    "        senA_word_ids += [0] * (max_sentence_len - len(senA_word_ids))\n",
    "        senB_word_ids += [0] * (max_sentence_len - len(senB_word_ids))\n",
    "               \n",
    "        assert(len(senA_word_ids) == len(senB_word_ids))\n",
    "        \n",
    "        # pad values to max_word_len and pad each sentence to max_sentence_len\n",
    "        senA_char_ids_wl = [word + ([0] * (max_word_len - len(word))) for word in senA_char_ids]\n",
    "        senA_char_ids_sl = [([0] * (max_word_len))] * (max_sentence_len - len(senA_char_ids_wl))\n",
    "        senA_char_ids = senA_char_ids_wl + senA_char_ids_sl\n",
    "\n",
    "        senB_char_ids_wl = [word + ([0] * (max_word_len - len(word))) for word in senB_char_ids]\n",
    "        senB_char_ids_sl = [([0] * (max_word_len))] * (max_sentence_len - len(senB_char_ids_wl))\n",
    "        senB_char_ids = senB_char_ids_wl + senB_char_ids_sl\n",
    "\n",
    "        # pad word lengths to max_sentence_len\n",
    "        senA_words_len = senA_words_len + ([0] * (max_sentence_len - len(senA_words_len)))\n",
    "        senB_words_len = senB_words_len + ([0] * (max_sentence_len - len(senB_words_len)))\n",
    "        \n",
    "        dataset.append((senA_word_ids, senA_len, senA_char_ids, senA_words_len, senB_word_ids, senB_len, senB_char_ids, senB_words_len, relatedness_scores, sentencesA, sentencesB))\n",
    "    return dataset\n",
    "\n",
    "print(\"pad_to_max_len_sentence_and_word_len\")\n",
    "\n",
    "\n",
    "for file in training_files:\n",
    "    print(file)\n",
    "    data_train, max_sentence_len_train, max_word_len_train = read_dataset(FLAGS.train_data+file, False, False, False, word2id, char2id)\n",
    "    '''\n",
    "    Get the max_sentence_len and max_word_len\n",
    "    '''\n",
    "    ## Check the sentence\n",
    "    # max_sentence_len = sorted(([(len(sentence), sentence) for sentence in sentences]))[-1]\n",
    "    max_sentence_len = max(max_sentence_len_train, max_sentence_len_test, max_sentence_len_dev)\n",
    "    print(max_sentence_len)\n",
    "\n",
    "    ## Check the word\n",
    "    # max_word_len = sorted(([(len(word), word) for word in word2id]))[-1]\n",
    "    max_word_len = max(max_word_len_train, max_word_len_test, max_word_len_dev)\n",
    "    print(max_word_len)\n",
    "\n",
    "\n",
    "    data_train = pad_to_max_len_sentence_and_word_len(data_train, max_sentence_len, max_word_len)\n",
    "\n",
    "    data_dev = pad_to_max_len_sentence_and_word_len(data_dev, max_sentence_len, max_word_len)\n",
    "    data_test = pad_to_max_len_sentence_and_word_len(data_test, max_sentence_len, max_word_len)\n",
    "\n",
    "    zipped_train = zip(*data_train)\n",
    "    zipped_dev = zip(*data_dev)\n",
    "    zipped_test = zip(*data_test)\n",
    "    len(zipped_train[3][0])\n",
    "    print(\"training..\")\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        initializer=tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        with tf.variable_scope('Model',initializer=initializer):\n",
    "\n",
    "            labels = tf.placeholder(tf.float32, shape=([None]),name='relatedness_score_label')\n",
    "            dropout_f = tf.placeholder(tf.float32)\n",
    "\n",
    "            #character embedding\n",
    "            # C (131, 50)\n",
    "            C = tf.Variable(tf.random_uniform([char_vocab_size, FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"char_embeddings\")\n",
    "            # sentences_words_A (30, 32, 16)\n",
    "            sentences_words_A = tf.placeholder(tf.int32, shape = ([None, max_sentence_len, max_word_len]), name='sentences_words_A')\n",
    "            # sentencesA_words_length (30, 32)\n",
    "            sentencesA_words_length = tf.placeholder(tf.int32, shape=([None, max_sentence_len]),name='sentencesA_words_length')    \n",
    "\n",
    "            sentences_words_B = tf.placeholder(tf.int32, shape = ([None, max_sentence_len, max_word_len]), name='sentences_words_B')\n",
    "            sentencesB_words_length = tf.placeholder(tf.int32, shape=([None, max_sentence_len]),name='sentencesB_words_length')    \n",
    "\n",
    "            # sentences_A_char_emb (30, 32, 16, 50)\n",
    "            sentences_A_char_emb = tf.nn.embedding_lookup(params = C, ids = sentences_words_A)\n",
    "            sentences_B_char_emb = tf.nn.embedding_lookup(params = C, ids = sentences_words_B)\n",
    "\n",
    "            #reshape char embedding\n",
    "            # sentences_A_char_emb_1 (960, 16, 50)\n",
    "            sentences_A_char_emb_1 = tf.reshape(sentences_A_char_emb, shape = ([-1, max_word_len, FLAGS.char_embedding_dim]))\n",
    "            # sentencesA_words_length_1 (960,)\n",
    "            sentencesA_words_length_1 = tf.reshape(sentencesA_words_length, shape = ([-1]))\n",
    "            sentences_B_char_emb_1 = tf.reshape(sentences_B_char_emb, shape = ([-1, max_word_len, FLAGS.char_embedding_dim]))\n",
    "            sentencesB_words_length_1 = tf.reshape(sentencesB_words_length, shape = ([-1]))\n",
    "\n",
    "            with tf.variable_scope('siamese_char') as scope:\n",
    "                # feed to biLSTM\n",
    "                c_outputs_A,c_last_states_A = build_model(sentences_A_char_emb_1,sentencesA_words_length_1,dropout_f)\n",
    "                scope.reuse_variables()\n",
    "                c_outputs_B,c_last_states_B = build_model(sentences_B_char_emb_1,sentencesB_words_length_1,dropout_f)\n",
    "\n",
    "                with tf.variable_scope('siamese_char_ff_a') as scope:\n",
    "                    # 50 x 50\n",
    "                    C_ff_weights_a = tf.Variable(tf.random_uniform([config.num_units, FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"C_ff_weights_a\")\n",
    "                    # 50\n",
    "                    C_ff_bias_a = tf.Variable(tf.random_uniform([FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"C_ff_bias_a\")\n",
    "                    # 960 x 50 * 50 x 50 \n",
    "                    c_output_a = tf.add(tf.matmul(c_last_states_A[0].h, C_ff_weights_a), C_ff_bias_a)\n",
    "                    # 960 x 16 x 50\n",
    "                    c_output_a = tf.tanh(c_output_a)\n",
    "                    # ? x 32 x 16 x 50\n",
    "                    c_output_a = tf.reshape(c_output_a, shape = ([-1, max_sentence_len, FLAGS.char_embedding_dim]))\n",
    "\n",
    "                with tf.variable_scope('siamese_char_ff_b') as scope:\n",
    "                    C_ff_weights_b = tf.Variable(tf.random_uniform([config.num_units, FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"C_ff_weights_b\")\n",
    "                    C_ff_bias_b = tf.Variable(tf.random_uniform([FLAGS.char_embedding_dim], -1.0, 1.0),trainable=True,name=\"C_ff_bias_b\")\n",
    "                    c_output_b = tf.add(tf.matmul(c_last_states_B[0].h, C_ff_weights_b), C_ff_bias_b)\n",
    "                    c_output_b = tf.tanh(c_output_b)\n",
    "                    c_output_b = tf.reshape(c_output_b, shape = ([-1, max_sentence_len, FLAGS.char_embedding_dim]))\n",
    "\n",
    "            prediction = tf.exp(tf.mul(-1.0,tf.reduce_mean(tf.reduce_mean(tf.abs(tf.sub(c_output_b,c_output_a)),1),1)))\n",
    "\n",
    "            cost = tf.reduce_mean(tf.square(tf.sub(prediction, labels)))\n",
    "\n",
    "            lr = tf.Variable(0.0,trainable=False)\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads,_ = tf.clip_by_global_norm(tf.gradients(cost,tvars),config.max_grad_norm)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            train_op = optimizer.apply_gradients(zip(grads,tvars),global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "            new_lr = tf.placeholder(tf.float32,shape=[],name='new_learning_rate')\n",
    "            lr_update = tf.assign(lr,new_lr)\n",
    "\n",
    "            for v in tf.trainable_variables():\n",
    "                print(\">>:\", v.name)\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            ## Launch training graph\n",
    "            with tf.Session(config=config_gpu) as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                total_batch = int(len(zipped_train[0]) / config.batch_size)##this doesn't include the extra samples? It does. see below after the \"if\" block.\n",
    "                print('Total batch size: {}, data size: {}, batch size: {}'.format(total_batch,len(zipped_train[0]),config.batch_size))\n",
    "                print(config.max_grad_norm,config.keep_prob,config.lr_decay,config.lr_max_epoch,config.train_max_epoch,config.num_layer)\n",
    "                # train\n",
    "                prev_train_cost=1\n",
    "                prev_valid_cost=1\n",
    "\n",
    "                patience = config.patience\n",
    "                train_costs = []\n",
    "                valid_costs = []\n",
    "\n",
    "\n",
    "                for epoch in range(config.train_max_epoch):\n",
    "                    lr_decay = config.lr_decay**max(epoch+1-config.lr_max_epoch,0.0)\n",
    "                    sess.run([lr,lr_update],feed_dict = {new_lr:config.learning_rate*lr_decay})\n",
    "\n",
    "                    print('Epoch {} Learning rate: {}'.format(epoch,sess.run(lr)))\n",
    "\n",
    "                    avg_cost=0.\n",
    "\n",
    "                    for i in range(total_batch):\n",
    "#                         print(\"i:\", i)\n",
    "                        start = i*config.batch_size\n",
    "                        end = (i+1)*config.batch_size\n",
    "                        next_batch_input = next_batch(start,end,zipped_train)\n",
    "\n",
    "                        _,train_cost= sess.run([train_op,cost], feed_dict={   \n",
    "                                sentences_words_A: next_batch_input.senA_char_ids,\n",
    "                                sentencesA_words_length: next_batch_input.senA_words_len,\n",
    "                                sentences_words_B: next_batch_input.senB_char_ids,\n",
    "                                sentencesB_words_length: next_batch_input.senB_words_len,\n",
    "                                labels: next_batch_input.relatedness_scores,\n",
    "                                dropout_f: config.keep_prob\n",
    "                            })\n",
    "                        avg_cost += train_cost\n",
    "\n",
    "\n",
    "\n",
    "                    start = total_batch*config.batch_size\n",
    "                    end = len(zipped_train[0])\n",
    "                    #check if the last trailing batch and handle it\n",
    "                    if not start == end:\n",
    "                        next_batch_input = next_batch(start,end,zipped_train)\n",
    "                        _,train_cost= sess.run([train_op,cost], feed_dict={                                         \n",
    "                                sentences_words_A: next_batch_input.senA_char_ids,\n",
    "                                sentencesA_words_length: next_batch_input.senA_words_len,\n",
    "                                sentences_words_B: next_batch_input.senB_char_ids,\n",
    "                                sentencesB_words_length: next_batch_input.senB_words_len,\n",
    "                                labels: next_batch_input.relatedness_scores,\n",
    "                                dropout_f: config.keep_prob\n",
    "                            })\n",
    "                        avg_cost += train_cost\n",
    "\n",
    "                    if prev_train_cost >  avg_cost / total_batch: \n",
    "                        print('Average cost:\\t{} >'.format(avg_cost / total_batch))\n",
    "                    else: \n",
    "                        print('Average cost:\\t{} <'.format(avg_cost / total_batch))\n",
    "\n",
    "                    prev_train_cost = avg_cost / total_batch\n",
    "\n",
    "                    train_costs.append(prev_train_cost)\n",
    "\n",
    "\n",
    "                    # validation\n",
    "                    next_batch_input = next_batch(0, len(zipped_dev[0]), zipped_dev)\n",
    "                    valid_cost,valid_predict=sess.run([cost,prediction],feed_dict={\n",
    "                            sentences_words_A: next_batch_input.senA_char_ids,\n",
    "                            sentencesA_words_length: next_batch_input.senA_words_len,\n",
    "                            sentences_words_B: next_batch_input.senB_char_ids,\n",
    "                            sentencesB_words_length: next_batch_input.senB_words_len,\n",
    "                            labels: next_batch_input.relatedness_scores,\n",
    "                            dropout_f: config.keep_prob\n",
    "                    })\n",
    "                    if prev_valid_cost > valid_cost: \n",
    "                        print('Valid cost:\\t{} >'.format(valid_cost))\n",
    "                    else: \n",
    "                        print('Valid cost:\\t{} <'.format(valid_cost))\n",
    "                    prev_valid_cost=valid_cost\n",
    "\n",
    "                    valid_costs.append(valid_cost)\n",
    "\n",
    "                    # early stopping\n",
    "                    if patience == 0:\n",
    "                        print(\"Lost patience:\", patience)\n",
    "                        break\n",
    "                    if prev_valid_cost > valid_cost:\n",
    "                        patience -= 1\n",
    "                    else:\n",
    "                        patience = 5\n",
    "                    print(\"patience:\", patience)\n",
    "                saver.save(sess, FLAGS.save_path+FLAGS.model_name+file,global_step=config.train_max_epoch)\n",
    "\n",
    "                # test\n",
    "                next_batch_input = next_batch(0, len(zipped_test[0]), zipped_test)\n",
    "                test_cost,test_predict=sess.run([cost,prediction],feed_dict={\n",
    "                        sentences_words_A: next_batch_input.senA_char_ids,\n",
    "                        sentencesA_words_length: next_batch_input.senA_words_len,\n",
    "                        sentences_words_B: next_batch_input.senB_char_ids,\n",
    "                        sentencesB_words_length: next_batch_input.senB_words_len,\n",
    "                        labels: next_batch_input.relatedness_scores,\n",
    "                        dropout_f: config.keep_prob       \n",
    "                })\n",
    "                print(test_cost)\n",
    "\n",
    "    \n",
    "    filename = FLAGS.result_path + FLAGS.model_name+'_'+file\n",
    "\n",
    "    with open(filename+'.txt','w') as fw:\n",
    "        labels = []\n",
    "        preds = []\n",
    "        for _ in range(len(test_predict)):\n",
    "            fw.write(str(next_batch_input.relatedness_scores[_])+'\\t'+str(test_predict[_])+'\\n')\n",
    "            labels.append(next_batch_input.relatedness_scores[_])\n",
    "            preds.append(test_predict[_])\n",
    "        print(\"Pearson:\", pearsonr(labels,preds))\n",
    "    stop = timeit.default_timer()\n",
    "    print(\"Execution time:\", stop - start)\n",
    "\n",
    "\n",
    "    y1 = train_costs\n",
    "    y2 = valid_costs\n",
    "    x_axis = range(1, len(y1)+1)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis, y1, label='Train Cost')\n",
    "    ax.plot(x_axis, y2, label='Validation Cost')\n",
    "    ax.legend()\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Training vs Validation Cost')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.savefig(filename+\".png\")\n",
    "    # plt.show()\n",
    "\n",
    "    test_data = read_input_file(FLAGS.test_data)\n",
    "    cnt = int(FLAGS.cnt)\n",
    "    with open(filename+'.txt', 'r') as f:\n",
    "        a = []\n",
    "        b = []\n",
    "        for line in f:\n",
    "            a.append(float(line.strip().split('\\t')[0]))\n",
    "            b.append(float(line.strip().split('\\t')[1]))\n",
    "        #most dissimilar/similar actual/predictions values\n",
    "        res = [abs(a[i] - b[i]) for i in range(len(a))]\n",
    "        sort = sorted(range(len(res)), key=lambda k: res[k], reverse = True)\n",
    "        firstn = sort[0:cnt]\n",
    "        lastn = sort[-(cnt):len(sort)]\n",
    "        lastn = lastn[::-1]\n",
    "\n",
    "        #prediction scores\n",
    "        predscores = sorted(range(len(b)), key=lambda k: b[k], reverse = True)\n",
    "        highpreds = predscores[0:cnt]\n",
    "        lowpreds = predscores[-(cnt):len(predscores)]\n",
    "        lowpreds = lowpreds[::-1]\n",
    "    with open(filename+'_samples.txt', 'w') as f:\n",
    "    #     print(\"Most dissimilar (actual - predicted)\\n\")\n",
    "        f.write(\"Most dissimilar (actual - predicted)\\n\")\n",
    "        for i in range(len(test_data.sentences_A)):\n",
    "            txt = str(next_batch_input.relatedness_scores[firstn[i]])+\"\\t\"+str(test_predict[firstn[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[firstn[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[firstn[i]])) +\"\\t\"+ str(zipped_test[9][firstn[i]])+\"\\t\"+str(zipped_test[10][firstn[i]])\n",
    "#             print(txt)\n",
    "            f.write(txt+\"\\n\")\n",
    "            if i == cnt-1:\n",
    "                break\n",
    "\n",
    "\n",
    "    #     print(\"Most similar (actual - predicted)\\n\")\n",
    "        f.write(\"\\nMost similar (actual - predicted)\\n\")\n",
    "        length = len(test_data.sentences_A)\n",
    "        for i in range(length):\n",
    "        #     print(i)\n",
    "            txt = str(next_batch_input.relatedness_scores[lastn[i]])+\"\\t\"+str(test_predict[lastn[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[lastn[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[lastn[i]]))+\"\\t\"+ str(zipped_test[9][lastn[i]])+\"\\t\"+str(zipped_test[10][lastn[i]])\n",
    "    #         print(txt)\n",
    "            f.write(txt+\"\\n\")\n",
    "            if i == cnt-1:\n",
    "                break\n",
    "    #     print(\"Most dissimilar sentence pairs as predicted\\n\")\n",
    "        f.write(\"\\nMost dissimilar sentence pairs as predicted\\n\")\n",
    "        for i in range(len(test_data.sentences_A)):\n",
    "        #     print(i)\n",
    "            txt = str(next_batch_input.relatedness_scores[lowpreds[i]])+\"\\t\"+str(test_predict[lowpreds[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[lowpreds[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[lowpreds[i]]))+\"\\t\"+ str(zipped_test[9][lowpreds[i]])+\"\\t\"+str(zipped_test[10][lowpreds[i]])\n",
    "    #         print(txt)\n",
    "            f.write(txt+\"\\n\")\n",
    "            if i == cnt-1:\n",
    "                break\n",
    "\n",
    "\n",
    "    #     print(\"Most similar sentence pairs as predicted\\n\")\n",
    "        f.write(\"\\nMost similar sentence pairs as predicted\\n\")\n",
    "        length = len(test_data.sentences_A)\n",
    "        for i in range(length):\n",
    "        #     print(i)\n",
    "            txt = str(next_batch_input.relatedness_scores[highpreds[i]])+\"\\t\"+str(test_predict[highpreds[i]])+\"\\t\"+ str(\" \".join(test_data.sentences_A[highpreds[i]]))+\"\\t\"+str(\" \".join(test_data.sentences_B[highpreds[i]]))+\"\\t\"+ str(zipped_test[9][highpreds[i]])+\"\\t\"+str(zipped_test[10][highpreds[i]])\n",
    "    #         print(txt)\n",
    "            f.write(txt+\"\\n\")\n",
    "            if i == cnt-1:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
